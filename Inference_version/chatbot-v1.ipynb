{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing necessary dependencies having the required versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T19:18:43.437123Z",
     "iopub.status.busy": "2025-03-18T19:18:43.436909Z",
     "iopub.status.idle": "2025-03-18T19:18:44.385014Z",
     "shell.execute_reply": "2025-03-18T19:18:44.384261Z",
     "shell.execute_reply.started": "2025-03-18T19:18:43.437101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-18T19:19:21.126764Z",
     "iopub.status.busy": "2025-03-18T19:19:21.126433Z",
     "iopub.status.idle": "2025-03-18T19:23:45.941610Z",
     "shell.execute_reply": "2025-03-18T19:23:45.940551Z",
     "shell.execute_reply.started": "2025-03-18T19:19:21.126735Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.45.2\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting vllm==0.6.0\n",
      "  Downloading vllm-0.6.0-cp38-abi3-manylinux1_x86_64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.4.5)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n",
      "  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (5.9.5)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (0.2.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (9.0.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (3.20.3)\n",
      "Collecting fastapi (from vllm==0.6.0)\n",
      "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (3.11.12)\n",
      "Requirement already satisfied: openai>=1.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (1.57.4)\n",
      "Collecting uvicorn[standard] (from vllm==0.6.0)\n",
      "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=2.8 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (2.11.0a2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (11.0.0)\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (0.21.1)\n",
      "Collecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.0)\n",
      "  Downloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (0.9.0)\n",
      "Collecting lm-format-enforcer==0.10.6 (from vllm==0.6.0)\n",
      "  Downloading lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting outlines<0.1,>=0.0.43 (from vllm==0.6.0)\n",
      "  Downloading outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (4.12.2)\n",
      "Collecting partial-json-parser (from vllm==0.6.0)\n",
      "  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (24.0.1)\n",
      "Collecting msgspec (from vllm==0.6.0)\n",
      "  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting gguf==0.9.1 (from vllm==0.6.0)\n",
      "  Downloading gguf-0.9.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (8.5.0)\n",
      "Collecting mistral-common>=1.3.4 (from vllm==0.6.0)\n",
      "  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: ray>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (2.42.1)\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (12.570.86)\n",
      "Collecting torch==2.4.0 (from vllm==0.6.0)\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchvision==0.19 (from vllm==0.6.0)\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Collecting xformers==0.0.27.post2 (from vllm==0.6.0)\n",
      "  Downloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Collecting vllm-flash-attn==2.6.1 (from vllm==0.6.0)\n",
      "  Downloading vllm_flash_attn-2.6.1-cp310-cp310-manylinux1_x86_64.whl.metadata (476 bytes)\n",
      "Collecting interegular>=0.3.2 (from lm-format-enforcer==0.10.6->vllm==0.6.0)\n",
      "  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0->vllm==0.6.0)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm==0.6.0) (12.6.85)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.3.4->vllm==0.6.0) (4.23.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (2.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (1.3.1)\n",
      "Collecting lark (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (1.6.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (3.1.0)\n",
      "Collecting diskcache (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (0.60.0)\n",
      "Requirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (0.35.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (3.3.1)\n",
      "Collecting pycountry (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyairports (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n",
      "  Downloading pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting starlette<1.0.0,>=0.30.0 (from prometheus-fastapi-instrumentator>=7.0.0->vllm==0.6.0)\n",
      "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8->vllm==0.6.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8->vllm==0.6.0) (2.29.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.0) (8.1.7)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.0) (1.1.0)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.0) (1.3.2)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.0) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2025.1.31)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (2.4.6)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (25.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (1.18.3)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->vllm==0.6.0) (3.21.0)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.0) (0.14.0)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]->vllm==0.6.0)\n",
      "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]->vllm==0.6.0)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm==0.6.0)\n",
      "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]->vllm==0.6.0)\n",
      "  Downloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.0) (14.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0->vllm==0.6.0) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0->vllm==0.6.0) (1.0.7)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.3.4->vllm==0.6.0) (2024.10.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.3.4->vllm==0.6.0) (0.22.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (0.70.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->vllm==0.6.0) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.45.2) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.45.2) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.45.2) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.45.2) (2024.2.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm==0.6.0) (0.43.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->vllm==0.6.0) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers==4.45.2) (2024.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (1.17.0)\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading vllm-0.6.0-cp38-abi3-manylinux1_x86_64.whl (170.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.6/170.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gguf-0.9.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lm_format_enforcer-0.10.6-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading vllm_flash_attn-2.6.1-cp310-cp310-manylinux1_x86_64.whl (75.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading outlines-0.0.46-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\n",
      "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyairports-2.1.1-py3-none-any.whl (371 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyairports, uvloop, uvicorn, triton, python-dotenv, pycountry, partial-json-parser, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, lark, interegular, httptools, diskcache, watchfiles, starlette, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, tokenizers, prometheus-fastapi-instrumentator, lm-format-enforcer, fastapi, vllm-flash-attn, xformers, transformers, torchvision, outlines, mistral-common, gguf, vllm\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
      "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.0\n",
      "    Uninstalling transformers-4.47.0:\n",
      "      Successfully uninstalled transformers-4.47.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1+cu121\n",
      "    Uninstalling torchvision-0.20.1+cu121:\n",
      "      Successfully uninstalled torchvision-0.20.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed diskcache-5.6.3 fastapi-0.115.11 gguf-0.9.1 httptools-0.6.4 interegular-0.3.3 lark-1.2.2 lm-format-enforcer-0.10.6 mistral-common-1.5.4 msgspec-0.19.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 outlines-0.0.46 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.0.2 pyairports-2.1.1 pycountry-24.6.1 python-dotenv-1.0.1 starlette-0.46.1 tokenizers-0.20.3 torch-2.4.0 torchvision-0.19.0 transformers-4.45.2 triton-3.0.0 uvicorn-0.34.0 uvloop-0.21.0 vllm-0.6.0 vllm-flash-attn-2.6.1 watchfiles-1.0.4 xformers-0.0.27.post2\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.4.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl (798.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.19.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.4.1\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.1) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.1) (11.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (2.4.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.19.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.19.1) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.19.1) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision==0.19.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision==0.19.1) (2024.2.0)\n",
      "Installing collected packages: torch, torchaudio, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.4.0\n",
      "    Uninstalling torch-2.4.0:\n",
      "      Successfully uninstalled torch-2.4.0\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.5.1+cu121\n",
      "    Uninstalling torchaudio-2.5.1+cu121:\n",
      "      Successfully uninstalled torchaudio-2.5.1+cu121\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.19.0\n",
      "    Uninstalling torchvision-0.19.0:\n",
      "      Successfully uninstalled torchvision-0.19.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.6.0 requires torch==2.4.0, but you have torch 2.4.1+cu121 which is incompatible.\n",
      "vllm 0.6.0 requires torchvision==0.19, but you have torchvision 0.19.1+cu121 which is incompatible.\n",
      "vllm-flash-attn 2.6.1 requires torch==2.4.0, but you have torch 2.4.1+cu121 which is incompatible.\n",
      "xformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.4.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121\n",
      "Found existing installation: pynvml 12.0.0\n",
      "Uninstalling pynvml-12.0.0:\n",
      "  Successfully uninstalled pynvml-12.0.0\n",
      "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (12.570.86)\n",
      "CPU times: user 4.48 s, sys: 1.07 s, total: 5.54 s\n",
      "Wall time: 4min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "! pip install -U transformers==4.45.2 vllm==0.6.0\n",
    "! pip install -U torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "! pip uninstall -y pynvml\n",
    "! pip install nvidia-ml-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T19:24:07.732975Z",
     "iopub.status.busy": "2025-03-18T19:24:07.732637Z",
     "iopub.status.idle": "2025-03-18T19:24:33.524843Z",
     "shell.execute_reply": "2025-03-18T19:24:33.524190Z",
     "shell.execute_reply.started": "2025-03-18T19:24:07.732948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Problem class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T19:27:36.357328Z",
     "iopub.status.busy": "2025-03-18T19:27:36.356967Z",
     "iopub.status.idle": "2025-03-18T19:27:36.361624Z",
     "shell.execute_reply": "2025-03-18T19:27:36.360951Z",
     "shell.execute_reply.started": "2025-03-18T19:27:36.357304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Problem:\n",
    "    def __init__(self, name, subject, subtopic, question, hint, solution, explanation):\n",
    "        self.name = name\n",
    "        self.subject = subject\n",
    "        self.subtopic = subtopic\n",
    "        self.question = question\n",
    "        self.hint = hint\n",
    "        self.solution = solution\n",
    "        self.explanation = explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T19:25:04.307049Z",
     "iopub.status.busy": "2025-03-18T19:25:04.306651Z",
     "iopub.status.idle": "2025-03-18T19:25:04.311197Z",
     "shell.execute_reply": "2025-03-18T19:25:04.310399Z",
     "shell.execute_reply.started": "2025-03-18T19:25:04.307017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "TENSOR_PARALLEL_SIZE = 2\n",
    "GPU_MEMORY_UTILIZATION = 0.90\n",
    "DTYPE = \"half\"\n",
    "MAX_MODEL_LEN = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T19:00:51.491015Z",
     "iopub.status.busy": "2025-03-18T19:00:51.490747Z",
     "iopub.status.idle": "2025-03-18T19:00:51.861253Z",
     "shell.execute_reply": "2025-03-18T19:00:51.860392Z",
     "shell.execute_reply.started": "2025-03-18T19:00:51.490993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model using VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T19:25:06.068332Z",
     "iopub.status.busy": "2025-03-18T19:25:06.068037Z",
     "iopub.status.idle": "2025-03-18T19:26:34.318296Z",
     "shell.execute_reply": "2025-03-18T19:26:34.317417Z",
     "shell.execute_reply.started": "2025-03-18T19:25:06.068310Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-18 19:25:06 config.py:890] Defaulting to use mp for distributed inference\n",
      "WARNING 03-18 19:25:06 config.py:378] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 03-18 19:25:06 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='meta-llama/Llama-2-7b-chat-hf', speculative_config=None, tokenizer='meta-llama/Llama-2-7b-chat-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-2-7b-chat-hf, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97281c897ae940f2bc9593c951b3ce83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192599cb480e43a2b847959941fb6877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61339f3a0da4a82ad91c25f2d404ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084ccd2f0558406e9feb749528db353b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d57d43364c43fcb202a25e0a80a3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-18 19:25:08 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-18 19:25:08 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 03-18 19:25:09 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-18 19:25:09 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:09 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:09 selector.py:116] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:09 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 03-18 19:25:10 utils.py:977] Found nccl from library libnccl.so.2\n",
      "INFO 03-18 19:25:10 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:10 utils.py:977] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:10 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 03-18 19:25:10 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 03-18 19:25:30 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:30 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 03-18 19:25:30 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7c8350a728c0>, local_subscribe_port=56953, remote_subscribe_port=None)\n",
      "INFO 03-18 19:25:30 model_runner.py:915] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:30 model_runner.py:915] Starting to load model meta-llama/Llama-2-7b-chat-hf...\n",
      "INFO 03-18 19:25:30 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-18 19:25:30 selector.py:116] Using XFormers backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:30 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:30 selector.py:116] Using XFormers backend.\n",
      "INFO 03-18 19:25:30 weight_utils.py:236] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:25:30 weight_utils.py:236] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879a73059db143708e24781a0bf06a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968a048f72a8405094f41175eb6b3b2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cd29ffd6764afbb8ead9f0243b2b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559e03fc19d1407e82a66bf87f01152a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-18 19:26:22 model_runner.py:926] Loading model weights took 6.3101 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=143)\u001b[0;0m INFO 03-18 19:26:27 model_runner.py:926] Loading model weights took 6.3101 GB\n",
      "INFO 03-18 19:26:30 distributed_gpu_executor.py:57] # GPU blocks: 1631, # CPU blocks: 1024\n"
     ]
    }
   ],
   "source": [
    "gllm = vllm.LLM(\n",
    "            MODEL_NAME,\n",
    "            tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "            gpu_memory_utilization=GPU_MEMORY_UTILIZATION, \n",
    "            trust_remote_code=True,\n",
    "            dtype=DTYPE, \n",
    "            enforce_eager=True,\n",
    "            max_model_len=MAX_MODEL_LEN,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T19:26:50.166224Z",
     "iopub.status.busy": "2025-03-18T19:26:50.165809Z",
     "iopub.status.idle": "2025-03-18T19:26:50.170272Z",
     "shell.execute_reply": "2025-03-18T19:26:50.169315Z",
     "shell.execute_reply.started": "2025-03-18T19:26:50.166196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer = gllm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatBot: Handles the interaction between the student and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T19:30:02.547018Z",
     "iopub.status.busy": "2025-03-18T19:30:02.546602Z",
     "iopub.status.idle": "2025-03-18T19:30:02.557351Z",
     "shell.execute_reply": "2025-03-18T19:30:02.556395Z",
     "shell.execute_reply.started": "2025-03-18T19:30:02.546985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ChatbotV2:\n",
    "    def __init__(self, llm, tokenizer, max_model_len=4096):\n",
    "        self.SYSTEM_INSTRUCTIONS = (\n",
    "            \"You are a **teaching assistant** who **guides** students in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. \"\n",
    "            \"You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.\"\n",
    "            \"Keep responses **brief (<100 tokens), complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. \"\n",
    "            \"Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. \"\n",
    "            \"**DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.\"\n",
    "            \"**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). \"\n",
    "            \"Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\"\n",
    "        )\n",
    "\n",
    "        self.PEDAGOGICAL_INSTRUCTIONS = (\n",
    "            \"Use **cognitive scaffolding** and **social-emotional support** to guide math learning effectively. \"\n",
    "            \"**Feedback:** Encourage correct answers only when explanation is provided (e.g., 'Yes, 9 is correct! Well done.'). \"\n",
    "            \"**Hints:** Give explicit hints without revealing answers (e.g., 'Think about the distributive property...'). \"\n",
    "            \"**Instructing:** Direct students toward the right approach (e.g., 'Try factoring first.'). \"\n",
    "            \"**Explaining:** Clarify concepts (e.g., 'Multiplying exponents? Add them.'). \"\n",
    "            \"**Modeling:** Demonstrate reasoning step by step (e.g., 'First, distribute...'). \"\n",
    "            \"**Questioning:** Use open-ended prompts (e.g., 'What strategy simplifies this fraction?'). \"\n",
    "            \"**Social-emotional support:** Encourage perseverance (e.g., 'You're on the right track!'), normalize mistakes (e.g., 'Mistakes help us learn.'), and foster collaboration (e.g., 'Explain your reasoning to a classmate.'). \"\n",
    "            \"Ensure students actively **engage in problem-solving** through structured guidance, **never direct answers.**\"\n",
    "        )\n",
    "\n",
    "        self.llm = llm\n",
    "        self.tokenizer = tokenizer\n",
    "        self.MAX_MODEL_LEN = max_model_len\n",
    "        self.messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.SYSTEM_INSTRUCTIONS + \"\\n\" + self.PEDAGOGICAL_INSTRUCTIONS,\n",
    "            }\n",
    "        ]\n",
    "        self.sampling_params = vllm.SamplingParams(max_tokens=250, temperature=0.2, top_p=1)\n",
    "        #self.modifier = ChatBotModifier()\n",
    "\n",
    "    def log_conversation(self, role, text, filename=\"conversation_log.txt\"):\n",
    "        with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{role}: {text}\\n\")\n",
    "\n",
    "    def construct_prompt(self, problem):\n",
    "        return (\n",
    "            f\"Problem Name: {problem.name}\\n\"\n",
    "            f\"Subject area: {problem.subject}\\n\"\n",
    "            f\"Subtopic: {problem.subtopic}\\n\"\n",
    "            f\"Problem Statement: {problem.question}\\n\"\n",
    "            f\"Hints: {problem.hint}\\n\"\n",
    "            f\"Solution: {problem.solution}\\n\"\n",
    "            f\"Explanation: {problem.explanation}\\n\"\n",
    "        )\n",
    "\n",
    "    def manage_token_limit(self):\n",
    "        while len(self.tokenizer.encode(self.format_messages())) > self.MAX_MODEL_LEN:\n",
    "            if len(self.messages) > 1:\n",
    "                self.messages.pop(1)  # Keep the system message\n",
    "\n",
    "    def format_messages(self):\n",
    "        return \"\\n\".join(f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in self.messages)\n",
    "\n",
    "    def get_response(self, user_input, problem):\n",
    "        self.log_conversation(\"Student\", user_input)\n",
    "        \n",
    "        if len(self.messages) == 1:  # Add problem details only once\n",
    "            self.messages.append({\"role\": \"system\", \"content\": self.construct_prompt(problem)})\n",
    "\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.manage_token_limit()\n",
    "\n",
    "        text = self.format_messages()\n",
    "        responses = self.llm.generate(text, sampling_params=self.sampling_params)\n",
    "        print(responses)\n",
    "        reply = responses[0].outputs[0].text.strip()\n",
    "\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        self.log_conversation(\"Assistant\", reply)\n",
    "        return reply\n",
    "\n",
    "    def cleanup(self):\n",
    "        print(\"Cleaning up GPU memory...\")\n",
    "        del self.llm\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(5)\n",
    "        print(\"Cleanup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T20:06:30.009604Z",
     "iopub.status.busy": "2025-03-18T20:06:30.009297Z",
     "iopub.status.idle": "2025-03-18T20:06:30.019026Z",
     "shell.execute_reply": "2025-03-18T20:06:30.018055Z",
     "shell.execute_reply.started": "2025-03-18T20:06:30.009581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Chatbot:\n",
    "    def __init__(self, llm, tokenizer, problem, max_model_len=4096):\n",
    "        self.SYSTEM_INSTRUCTIONS = (\n",
    "            \"You are a **teaching assistant** who **guides** students in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. \"\n",
    "            \"You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.\"\n",
    "            \"Keep responses **brief (<100 tokens), complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. \"\n",
    "            \"Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. \"\n",
    "            \"**DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.\"\n",
    "            \"**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). \"\n",
    "            \"Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\"\n",
    "        )\n",
    "\n",
    "        self.PEDAGOGICAL_INSTRUCTIONS = (\n",
    "            \"Use **cognitive scaffolding** and **social-emotional support** to guide math learning effectively. \"\n",
    "            \"**Feedback:** Encourage correct answers only when explanation is provided (e.g., 'Yes, 9 is correct! Well done.'). \"\n",
    "            \"**Hints:** Give explicit hints without revealing answers (e.g., 'Think about the distributive property...'). \"\n",
    "            \"**Instructing:** Direct students toward the right approach (e.g., 'Try factoring first.'). \"\n",
    "            \"**Explaining:** Clarify concepts (e.g., 'Multiplying exponents? Add them.'). \"\n",
    "            \"**Modeling:** Demonstrate reasoning step by step (e.g., 'First, distribute...'). \"\n",
    "            \"**Questioning:** Use open-ended prompts (e.g., 'What strategy simplifies this fraction?'). \"\n",
    "            \"**Social-emotional support:** Encourage perseverance (e.g., 'You're on the right track!'), normalize mistakes (e.g., 'Mistakes help us learn.'), and foster collaboration (e.g., 'Explain your reasoning to a classmate.'). \"\n",
    "            \"Ensure students actively **engage in problem-solving** through structured guidance, **never direct answers.**\"\n",
    "        )\n",
    "\n",
    "        self.llm = llm\n",
    "        self.tokenizer = tokenizer\n",
    "        self.MAX_MODEL_LEN = max_model_len\n",
    "        self.messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": self.SYSTEM_INSTRUCTIONS + \"\\n\" + self.PEDAGOGICAL_INSTRUCTIONS +\n",
    "                (\n",
    "                    f\"\\nProblem Name: {problem.name}\\n\"\n",
    "                    f\"Subject Area: {problem.subject}\\n\"\n",
    "                    f\"Subtopic: {problem.subtopic}\\n\"\n",
    "                    f\"Problem Statement: {problem.question}\\n\"\n",
    "                    f\"Hints: {problem.hint}\\n\"\n",
    "                    f\"Solution: {problem.solution}\\n\"\n",
    "                    f\"Explanation: {problem.explanation}\\n\"\n",
    "                )\n",
    "            }\n",
    "        ]\n",
    "        self.sampling_params = vllm.SamplingParams(max_tokens=250, temperature=0.2, top_p=1)\n",
    "\n",
    "    def log_conversation(self, role, text, filename=\"conversation_log.txt\"):\n",
    "        \"\"\"Logs conversation history to a file.\"\"\"\n",
    "        with open(filename, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{role}: {text}\\n\")\n",
    "\n",
    "    def manage_token_limit(self):\n",
    "        \"\"\"Ensures conversation stays within the token limit by keeping only the latest system message and user input.\"\"\"\n",
    "        while self.tokenizer.num_tokens(self.messages) > self.MAX_MODEL_LEN:\n",
    "            if len(self.messages) > 2:\n",
    "                self.messages.pop(1)  # Remove older user input to maintain context\n",
    "            else:\n",
    "                break  # Ensure we always keep the system message\n",
    "\n",
    "    def get_response(self, user_input):\n",
    "        \"\"\"Processes user input and returns the chatbot's response.\"\"\"\n",
    "        self.log_conversation(\"Student\", user_input)\n",
    "        print(self.messages)\n",
    "        # Always keep only the system message + latest user input\n",
    "        self.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        print(self.messages)\n",
    "        responses = self.llm.chat(self.messages, sampling_params=self.sampling_params)\n",
    "\n",
    "        if responses and responses[0].outputs:\n",
    "            reply = responses[0].outputs[0].text.strip()\n",
    "        else:\n",
    "            reply = \"I'm sorry, I couldn't generate a response.\"\n",
    "    \n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        self.log_conversation(\"Assistant\", reply)\n",
    "        return reply\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleans up GPU memory usage.\"\"\"\n",
    "        print(\"Cleaning up GPU memory...\")\n",
    "        try:\n",
    "            del self.llm\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during cleanup: {e}\")\n",
    "        print(\"Cleanup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T20:06:31.719133Z",
     "iopub.status.busy": "2025-03-18T20:06:31.718760Z",
     "iopub.status.idle": "2025-03-18T20:06:31.723153Z",
     "shell.execute_reply": "2025-03-18T20:06:31.722288Z",
     "shell.execute_reply.started": "2025-03-18T20:06:31.719103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chatbot = Chatbot(gllm, tokenizer, problems[0], MAX_MODEL_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample problems to test with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T19:27:40.984738Z",
     "iopub.status.busy": "2025-03-18T19:27:40.984425Z",
     "iopub.status.idle": "2025-03-18T19:27:40.991799Z",
     "shell.execute_reply": "2025-03-18T19:27:40.990849Z",
     "shell.execute_reply.started": "2025-03-18T19:27:40.984712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "problems = [\n",
    "    Problem(\n",
    "        name=\"Bacteria Explosion\",\n",
    "        subject=\"Math\",\n",
    "        subtopic=\"Number Series\",\n",
    "        question=(\"Bweva was studying bacteriology (a biological field that studies microbes). \"\n",
    "                 \"She found that bacteria proliferated into two daughter cells. That is, 1 bacteria becomes 2 by dividing itself, \"\n",
    "                 \"then each of them divides into two more, making a total of 4, and so on. \"\n",
    "                 \"What is the number of produced bacteria after 5 proliferations, if initially the number of bacteria was 1?\"),\n",
    "        hint=\"Think about how the number of bacteria doubles each time. What does this tell you about the pattern?\",\n",
    "        solution=\"32\",\n",
    "        explanation=(\"As for every proliferation, the number doubles, it can be represented as 2^n (2 to the power n). \"\n",
    "                     \"Here n is the number of proliferations. For example, 2^3 = 2 * 2 * 2 = 8. \"\n",
    "                     \"This is an example of an exponential growth process where the value grows quickly as the number of proliferations increases.\")\n",
    "    ),\n",
    "    Problem(\n",
    "        name=\"Clock Overlap\",\n",
    "        subject=\"Math\",\n",
    "        subtopic=\"Clock\",\n",
    "        question=\"How many times in a day do the hour and minute hands of a clock overlap each other?\",\n",
    "        hint=\"\",\n",
    "        solution=\"22\",\n",
    "        explanation=(\"Let us consider a 12-hour time window starting from 12 PM to 12 AM. \"\n",
    "                     \"The hands overlap approximately at the following 11 times: 12:00PM, 1:05PM, 2:10PM, 3:16PM, 4:22PM, 5:27PM, \"\n",
    "                     \"6:33PM, 7:38PM, 8:43PM, 9:48PM, 10:54PM. \"\n",
    "                     \"Since in 12 hours the hands overlap 11 times, in 24 hours they overlap 22 times.\")\n",
    "    ),\n",
    "    Problem(\n",
    "        name=\"Instructor Course Distribution\",\n",
    "        subject=\"Math\",\n",
    "        subtopic=\"Algebra\",\n",
    "        question=(\"In a big programming boot camp with 600 students and 15 instructors, each student joins three different courses. \"\n",
    "                 \"There are 30 students in each course, and one instructor teaches each course. \"\n",
    "                 \"How many courses does each instructor teach?\"),\n",
    "        hint=\"\",\n",
    "        solution=\"4\",\n",
    "        explanation=(\"There are 600 students and 30 students in a course, so there must be at least 600/30 = 20 courses. \"\n",
    "                     \"Since each student participates in 3 different courses, there will be a total of 20 * 3 = 60 courses. \"\n",
    "                     \"As there are 15 instructors, each instructor will teach 60 / 15 = 4 courses.\")\n",
    "    ),\n",
    "    Problem(\n",
    "        name=\"Odd vs Even Sum\",\n",
    "        subject=\"Math\",\n",
    "        subtopic=\"Number Series\",\n",
    "        question=(\"Your friend Joey is fond of even numbers and you are fond of odd numbers. \"\n",
    "                 \"One morning at 10:01 AM, Joey asks you a question: \"\n",
    "                 \"Can you tell me the difference between the sum of odd numbers and the sum of even numbers up to 101 (included), if we start from 1?\"),\n",
    "        hint=\"\",\n",
    "        solution=\"51\",\n",
    "        explanation=(\"Look at the series: 1, 2, 3, 4, 5, ..., 101. \"\n",
    "                     \"We need to compute (1 + 3 + ... + 101) - (2 + 4 + ... + 100). \"\n",
    "                     \"Pairing them up: (1 - 2) + (3 - 4) + ... + (99 - 100) results in -50 (as there are 50 pairs). \"\n",
    "                     \"The last number, 101, has no pair, so the final sum is 101 - 50 = 51.\")\n",
    "    ),\n",
    "    Problem(\n",
    "        name=\"Watermelon Sharing\",\n",
    "        subject=\"Math\",\n",
    "        subtopic=\"Number Theory\",\n",
    "        question=(\"Aftab and Elias go to a supermarket to buy watermelons. They decide to buy X melons. \"\n",
    "                 \"However, both of them like even numbers. So they want to share the melons with each other such that \"\n",
    "                 \"both of them get an even number of melons. Both of them should get at least 1 melon, and they don't have to get an equal number of melons. \"\n",
    "                 \"They can afford to buy up to 100 melons. How many possible values of X exist to satisfy their condition?\"),\n",
    "        hint=\"\",\n",
    "        solution=\"49\",\n",
    "        explanation=(\"Both of them must get an even number of melons. The sum of two even numbers is always an even number, \"\n",
    "                     \"so X has to be even. However, X cannot be 2, as they each need at least 1 melon. \"\n",
    "                     \"Any even number greater than 2 can be written as a sum of two positive even numbers. \"\n",
    "                     \"Thus, X can be any even number greater than 2 but not exceeding 100. \"\n",
    "                     \"There are 50 even numbers between 1 and 100, and we exclude 2, leaving 49 valid values for X.\")\n",
    "    ),\n",
    "    Problem(\n",
    "        name=\"The Brave Knight of Zalora\",\n",
    "        subject=\"Logical Reasoning\",\n",
    "        subtopic=\"Greedy\",\n",
    "        question=(\"You are the brave knight of Zalora. You have entered the dark dungeon where four fearsome dragons are guarding a treasure. \"\n",
    "                  \"Two of them are asleep, and two of them are awake. You have to slay them all to claim the treasure. \"\n",
    "                  \"But beware, the dragons have a strange magic.\\n\"\n",
    "                  \"When you slay a sleeping dragon, it wakes up with a roar. If the dragon next to it is sleeping, it also wakes up. \"\n",
    "                  \"But if the dragon next to it is awake, it falls asleep.\\n\"\n",
    "                  \"Can you figure out how many dragons you have to slay to make all of them asleep?\"),\n",
    "        hint=\"\",\n",
    "        solution=\"3\",\n",
    "        explanation=(\"You have to slay three dragons.\\n\"\n",
    "                     \"Step 1: Slay the fourth dragon. The dragons change state.\\n\"\n",
    "                     \"Step 2: Slay the third dragon. The dragons change state again.\\n\"\n",
    "                     \"Step 3: Slay the second dragon. Finally, all dragons are asleep.\\n\"\n",
    "                     \"So, in total, you have to slay three dragons.\")\n",
    "    ),\n",
    "    Problem(\n",
    "        name=\"Fois and Cut - On Paper\",\n",
    "        subject=\"Number Theory\",\n",
    "        subtopic=\"Finding Digits\",\n",
    "        question=(\"In Kramp's magical world, he got two special spells!\\n\"\n",
    "                  \"With 'Fois X,' it tells you the result of multiplying all numbers from 1 to X (e.g., Fois 3 = 6).\\n\"\n",
    "                  \"With 'Cut Y,' it tells you how many zeroes are at the end of Y (e.g., Cut 25 = 0).\\n\"\n",
    "                  \"Kramp tests the spell by writing an expression. What magical number do you think Kramp will hear?\"),\n",
    "        hint=\"\",\n",
    "        solution=\"20\",\n",
    "        explanation=(\"The number of trailing zeros in Fois 95 is 22. For Fois 97, it is also 22, giving a total of 44 in the numerator.\\n\"\n",
    "                     \"For Fois 101, the count is 24.\\n\"\n",
    "                     \"Since the denominator has 24 zeros, we cancel them out, leaving 20 trailing zeroes in the final result.\")\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-18T20:15:22.855Z",
     "iopub.execute_input": "2025-03-18T20:15:02.421887Z",
     "iopub.status.busy": "2025-03-18T20:15:02.421467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  Can you please tell me the answer?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"You are a **teaching assistant** who **guides** students in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.Keep responses **brief (<100 tokens), complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. **DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\\nUse **cognitive scaffolding** and **social-emotional support** to guide math learning effectively. **Feedback:** Encourage correct answers only when explanation is provided (e.g., 'Yes, 9 is correct! Well done.'). **Hints:** Give explicit hints without revealing answers (e.g., 'Think about the distributive property...'). **Instructing:** Direct students toward the right approach (e.g., 'Try factoring first.'). **Explaining:** Clarify concepts (e.g., 'Multiplying exponents? Add them.'). **Modeling:** Demonstrate reasoning step by step (e.g., 'First, distribute...'). **Questioning:** Use open-ended prompts (e.g., 'What strategy simplifies this fraction?'). **Social-emotional support:** Encourage perseverance (e.g., 'You're on the right track!'), normalize mistakes (e.g., 'Mistakes help us learn.'), and foster collaboration (e.g., 'Explain your reasoning to a classmate.'). Ensure students actively **engage in problem-solving** through structured guidance, **never direct answers.**\\nProblem Name: Bacteria Explosion\\nSubject Area: Math\\nSubtopic: Number Series\\nProblem Statement: Bweva was studying bacteriology (a biological field that studies microbes). She found that bacteria proliferated into two daughter cells. That is, 1 bacteria becomes 2 by dividing itself, then each of them divides into two more, making a total of 4, and so on. What is the number of produced bacteria after 5 proliferations, if initially the number of bacteria was 1?\\nHints: Think about how the number of bacteria doubles each time. What does this tell you about the pattern?\\nSolution: 32\\nExplanation: As for every proliferation, the number doubles, it can be represented as 2^n (2 to the power n). Here n is the number of proliferations. For example, 2^3 = 2 * 2 * 2 = 8. This is an example of an exponential growth process where the value grows quickly as the number of proliferations increases.\\n\"}, {'role': 'user', 'content': 'Can you explain the problem?'}, {'role': 'assistant', 'content': 'Of course! I\\'d be happy to help you understand the problem \"Bacteria Explosion.\"\\n\\nSo, Bweva is studying bacteriology, and she found that bacteria can proliferate into two daughter cells. This means that one bacteria becomes two bacteria, and then each of those two bacteria becomes two more bacteria, and so on.\\n\\nThe problem asks you to calculate the number of produced bacteria after 5 proliferations, if initially there were 1 bacteria.\\n\\nCan you think about how the number of bacteria doubles each time? What does this tell you about the pattern?\\n\\nPlease let me know your thoughts, and I\\'ll provide you with a hint to help you solve the problem.'}, {'role': 'user', 'content': 'Can you please tell me the answer?'}, {'role': 'assistant', 'content': 'Of course! The answer to the problem \"Bacteria Explosion\" is 32.\\n\\nTo calculate the number of produced bacteria after 5 proliferations, you can use the pattern of the problem: each bacteria becomes 2 bacteria, so the number of bacteria doubles each time.\\n\\nHere\\'s how you can calculate it:\\n\\n1 bacteria x 2 = 2 bacteria\\n2 bacteria x 2 = 4 bacteria\\n3 bacteria x 2 = 8 bacteria\\n4 bacteria x 2 = 16 bacteria\\n5 bacteria x 2 = 32 bacteria\\n\\nSo, after 5 proliferations, there are 32 bacteria in total.\\n\\nI hope this helps! Let me know if you have any other questions.'}]\n",
      "[{'role': 'system', 'content': \"You are a **teaching assistant** who **guides** students in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.Keep responses **brief (<100 tokens), complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. **DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\\nUse **cognitive scaffolding** and **social-emotional support** to guide math learning effectively. **Feedback:** Encourage correct answers only when explanation is provided (e.g., 'Yes, 9 is correct! Well done.'). **Hints:** Give explicit hints without revealing answers (e.g., 'Think about the distributive property...'). **Instructing:** Direct students toward the right approach (e.g., 'Try factoring first.'). **Explaining:** Clarify concepts (e.g., 'Multiplying exponents? Add them.'). **Modeling:** Demonstrate reasoning step by step (e.g., 'First, distribute...'). **Questioning:** Use open-ended prompts (e.g., 'What strategy simplifies this fraction?'). **Social-emotional support:** Encourage perseverance (e.g., 'You're on the right track!'), normalize mistakes (e.g., 'Mistakes help us learn.'), and foster collaboration (e.g., 'Explain your reasoning to a classmate.'). Ensure students actively **engage in problem-solving** through structured guidance, **never direct answers.**\\nProblem Name: Bacteria Explosion\\nSubject Area: Math\\nSubtopic: Number Series\\nProblem Statement: Bweva was studying bacteriology (a biological field that studies microbes). She found that bacteria proliferated into two daughter cells. That is, 1 bacteria becomes 2 by dividing itself, then each of them divides into two more, making a total of 4, and so on. What is the number of produced bacteria after 5 proliferations, if initially the number of bacteria was 1?\\nHints: Think about how the number of bacteria doubles each time. What does this tell you about the pattern?\\nSolution: 32\\nExplanation: As for every proliferation, the number doubles, it can be represented as 2^n (2 to the power n). Here n is the number of proliferations. For example, 2^3 = 2 * 2 * 2 = 8. This is an example of an exponential growth process where the value grows quickly as the number of proliferations increases.\\n\"}, {'role': 'user', 'content': 'Can you explain the problem?'}, {'role': 'assistant', 'content': 'Of course! I\\'d be happy to help you understand the problem \"Bacteria Explosion.\"\\n\\nSo, Bweva is studying bacteriology, and she found that bacteria can proliferate into two daughter cells. This means that one bacteria becomes two bacteria, and then each of those two bacteria becomes two more bacteria, and so on.\\n\\nThe problem asks you to calculate the number of produced bacteria after 5 proliferations, if initially there were 1 bacteria.\\n\\nCan you think about how the number of bacteria doubles each time? What does this tell you about the pattern?\\n\\nPlease let me know your thoughts, and I\\'ll provide you with a hint to help you solve the problem.'}, {'role': 'user', 'content': 'Can you please tell me the answer?'}, {'role': 'assistant', 'content': 'Of course! The answer to the problem \"Bacteria Explosion\" is 32.\\n\\nTo calculate the number of produced bacteria after 5 proliferations, you can use the pattern of the problem: each bacteria becomes 2 bacteria, so the number of bacteria doubles each time.\\n\\nHere\\'s how you can calculate it:\\n\\n1 bacteria x 2 = 2 bacteria\\n2 bacteria x 2 = 4 bacteria\\n3 bacteria x 2 = 8 bacteria\\n4 bacteria x 2 = 16 bacteria\\n5 bacteria x 2 = 32 bacteria\\n\\nSo, after 5 proliferations, there are 32 bacteria in total.\\n\\nI hope this helps! Let me know if you have any other questions.'}, {'role': 'user', 'content': 'Can you please tell me the answer?'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.43s/it, est. speed input: 99.69 toks/s, output: 14.74 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! The answer to the problem \"Bacteria Explosion\" is 32.\n",
      "\n",
      "To calculate the number of produced bacteria after 5 proliferations, you can use the pattern of the problem: each bacteria becomes 2 bacteria, so the number of bacteria doubles each time.\n",
      "\n",
      "Here's how you can calculate it:\n",
      "\n",
      "1 bacteria x 2 = 2 bacteria\n",
      "2 bacteria x 2 = 4 bacteria\n",
      "3 bacteria x 2 = 8 bacteria\n",
      "4 bacteria x 2 = 16 bacteria\n",
      "5 bacteria x 2 = 32 bacteria\n",
      "\n",
      "So, after 5 proliferations, there are 32 bacteria in total.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Exiting chatbot...\")\n",
    "        break\n",
    "    response = chatbot.get_response(user_input)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chatbot.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T18:48:20.521959Z",
     "iopub.status.busy": "2025-03-18T18:48:20.521641Z",
     "iopub.status.idle": "2025-03-18T18:48:20.526537Z",
     "shell.execute_reply": "2025-03-18T18:48:20.525697Z",
     "shell.execute_reply.started": "2025-03-18T18:48:20.521936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chat = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a **teaching assistant** who **guides** students in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students. Keep responses **brief (<100 tokens), complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. **DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations. **ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\\n\\nUse **cognitive scaffolding** and **social-emotional support** to guide math learning effectively. **Feedback:** Encourage correct answers only when explanation is provided (e.g., 'Yes, 9 is correct! Well done.'). **Hints:** Give explicit hints without revealing answers (e.g., 'Think about the distributive property...'). **Instructing:** Direct students toward the right approach (e.g., 'Try factoring first.'). **Explaining:** Clarify concepts (e.g., 'Multiplying exponents? Add them.'). **Modeling:** Demonstrate reasoning step by step (e.g., 'First, distribute...'). **Questioning:** Use open-ended prompts (e.g., 'What strategy simplifies this fraction?'). **Social-emotional support:** Encourage perseverance (e.g., 'You're on the right track!'), normalize mistakes (e.g., 'Mistakes help us learn.'), and foster collaboration (e.g., 'Explain your reasoning to a classmate.'). Ensure students actively **engage in problem-solving** through structured guidance, **never direct answers.**\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"Problem Name: Bacteria Explosion\\nSubject area: Math\\nSubtopic: Number Series\\nProblem Statement: Bweva was studying bacteriology (a biological field that studies microbes). She found that bacteria proliferated into two daughter cells. That is, 1 bacteria becomes 2 by dividing itself, then each of them divides into two more, making a total of 4, and so on. What is the number of produced bacteria after 5 proliferations, if initially the number of bacteria was 1?\\n\\nHints: Think about how the number of bacteria doubles each time. What does this tell you about the pattern?\\n\\nSolution: 32\\n\\nExplanation: As for every proliferation, the number doubles, it can be represented as 2^n (2 to the power n). Here n is the number of proliferations. For example, 2^3 = 2 * 2 * 2 = 8. This is an example of an exponential growth process where the value grows quickly as the number of proliferations increases.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"How do I solve this problem?\"\n",
    "  }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4746046,
     "sourceId": 8300737,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
