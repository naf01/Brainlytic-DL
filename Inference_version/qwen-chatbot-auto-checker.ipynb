{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":8300737,"sourceType":"datasetVersion","datasetId":4746046}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing necessary dependencies having the required versions","metadata":{}},{"cell_type":"code","source":"%%time\n%%capture\n! pip install -U transformers==4.45.2 vllm==0.6.0\n! pip install -U torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n! pip uninstall -y pynvml\n! pip install nvidia-ml-py","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-03-22T18:21:22.572625Z","iopub.execute_input":"2025-03-22T18:21:22.572952Z","iopub.status.idle":"2025-03-22T18:25:22.780051Z","shell.execute_reply.started":"2025-03-22T18:21:22.572926Z","shell.execute_reply":"2025-03-22T18:25:22.779050Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 757 ms, sys: 194 ms, total: 951 ms\nWall time: 4min\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport gc\nimport vllm\nimport json\nimport time\nfrom tqdm import tqdm\nfrom datetime import datetime","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:31:23.704827Z","iopub.execute_input":"2025-03-22T18:31:23.705136Z","iopub.status.idle":"2025-03-22T18:31:23.709354Z","shell.execute_reply.started":"2025-03-22T18:31:23.705112Z","shell.execute_reply":"2025-03-22T18:31:23.708385Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# Defining a Problem class","metadata":{}},{"cell_type":"code","source":"class Problem:\n    def __init__(self, name, subject, subtopic, question, hint, solution, explanation):\n        self.name = name\n        self.subject = subject\n        self.subtopic = subtopic\n        self.question = question\n        self.hint = hint\n        self.solution = solution\n        self.explanation = explanation","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:25:44.173882Z","iopub.execute_input":"2025-03-22T18:25:44.174181Z","iopub.status.idle":"2025-03-22T18:25:44.178255Z","shell.execute_reply.started":"2025-03-22T18:25:44.174152Z","shell.execute_reply":"2025-03-22T18:25:44.177365Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"if True:\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:25:44.179587Z","iopub.execute_input":"2025-03-22T18:25:44.179933Z","iopub.status.idle":"2025-03-22T18:25:44.565045Z","shell.execute_reply.started":"2025-03-22T18:25:44.179903Z","shell.execute_reply":"2025-03-22T18:25:44.563924Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Defining model parameters","metadata":{}},{"cell_type":"code","source":"# Define model parameters\nTENSOR_PARALLEL_SIZE = 2\nGPU_MEMORY_UTILIZATION = 0.95\nDTYPE = \"half\"\nMAX_MODEL_LEN = 10240","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:25:44.566169Z","iopub.execute_input":"2025-03-22T18:25:44.566528Z","iopub.status.idle":"2025-03-22T18:25:44.578581Z","shell.execute_reply.started":"2025-03-22T18:25:44.566494Z","shell.execute_reply":"2025-03-22T18:25:44.577763Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"models = [\n    \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int8\",\n    \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\",\n    \"Qwen/Qwen1.5-14B-Chat-GPTQ-Int4\",\n    \"Qwen/Qwen-14B-Chat-Int4\"\n]","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:25:44.579593Z","iopub.execute_input":"2025-03-22T18:25:44.579940Z","iopub.status.idle":"2025-03-22T18:25:44.591208Z","shell.execute_reply.started":"2025-03-22T18:25:44.579910Z","shell.execute_reply":"2025-03-22T18:25:44.590475Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Loading the model using VLLM","metadata":{}},{"cell_type":"code","source":"gllm = vllm.LLM(\n            models[1],\n            tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n            gpu_memory_utilization=GPU_MEMORY_UTILIZATION, \n            trust_remote_code=True,\n            dtype=DTYPE, \n            enforce_eager=True,\n            max_model_len=MAX_MODEL_LEN,\n)","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:25:44.592097Z","iopub.execute_input":"2025-03-22T18:25:44.592353Z","iopub.status.idle":"2025-03-22T18:28:18.943282Z","shell.execute_reply.started":"2025-03-22T18:25:44.592333Z","shell.execute_reply":"2025-03-22T18:28:18.941942Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f38328feb1624e1bbc2ae0c608ac673a"}},"metadata":{}},{"name":"stdout","text":"WARNING 03-22 18:25:44 config.py:330] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 03-22 18:25:44 config.py:890] Defaulting to use mp for distributed inference\nWARNING 03-22 18:25:44 config.py:378] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 03-22 18:25:44 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=10240, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b2d38f53d64f73b8a1719e1e2e77f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a9b96eddd9045868db849db26fcc94c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc9cb0efe1d44420ae6a16d806a3c872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b16fa1dbb43645b8a9b476d86f4543dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e10b86d8f40b4894b059b7201b870726"}},"metadata":{}},{"name":"stdout","text":"WARNING 03-22 18:25:46 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 03-22 18:25:46 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\nINFO 03-22 18:25:46 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 03-22 18:25:46 selector.py:116] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:25:46 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:25:46 selector.py:116] Using XFormers backend.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:25:47 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\nINFO 03-22 18:25:48 utils.py:977] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:25:48 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 03-22 18:25:48 utils.py:977] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:25:48 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 03-22 18:25:48 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 03-22 18:26:07 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nWARNING 03-22 18:26:07 custom_all_reduce.py:131] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:26:07 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m WARNING 03-22 18:26:07 custom_all_reduce.py:131] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\nINFO 03-22 18:26:07 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7c6e98dbcf70>, local_subscribe_port=45281, remote_subscribe_port=None)\nINFO 03-22 18:26:07 model_runner.py:915] Starting to load model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4...\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:26:07 model_runner.py:915] Starting to load model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4...\nINFO 03-22 18:26:08 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 03-22 18:26:08 selector.py:116] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:26:08 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:26:08 selector.py:116] Using XFormers backend.\nINFO 03-22 18:26:08 weight_utils.py:236] Using model weights format ['*.safetensors']\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:26:08 weight_utils.py:236] Using model weights format ['*.safetensors']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0620c897f0294f41849fd57185dc4686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37c07ebfeb1f4b83ade242f0ac8b150f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a8441aff7fb4c498b855188cc42fc90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b8a6d8452341b8b65e99065d2c7290"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/3.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33d9d26b93cb4c6bb91431eddac6b2a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/172k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc014d87cf8a412a9ea315610480d5f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dd7353b7ee04e37a6a4580a1537674e"}},"metadata":{}},{"name":"stdout","text":"INFO 03-22 18:27:59 model_runner.py:926] Loading model weights took 9.0933 GB\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-22 18:27:59 model_runner.py:926] Loading model weights took 9.0933 GB\nINFO 03-22 18:28:14 distributed_gpu_executor.py:57] # GPU blocks: 1022, # CPU blocks: 2048\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: , Traceback (most recent call last):\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 69, in start_worker_execution_loop\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     output = self.execute_model(execute_model_req=None)\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 327, in execute_model\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     output = self.model_runner.execute_model(\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1450, in execute_model\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 361, in forward\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     hidden_states = self.model(input_ids, positions, kv_caches,\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 276, in forward\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     layer = self.layers[i]\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 294, in __getitem__\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226]     if isinstance(idx, slice):\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226] KeyboardInterrupt\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ERROR 03-22 18:39:06 multiproc_worker_utils.py:226] \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"tokenizer = gllm.get_tokenizer()","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:28:25.370289Z","iopub.execute_input":"2025-03-22T18:28:25.370581Z","iopub.status.idle":"2025-03-22T18:28:25.374503Z","shell.execute_reply.started":"2025-03-22T18:28:25.370559Z","shell.execute_reply":"2025-03-22T18:28:25.373377Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# ChatBot: Handles the interaction between the student and the model","metadata":{}},{"cell_type":"code","source":"class Chatbot:\n    def __init__(self, llm, top_p, temp, INS, tokenizer, problem, filename, intro):\n        self.top_p = top_p\n        self.temp = temp\n        self.SYSTEM_INSTRUCTION = INS[0]\n        self.RECHECKER_INSTRUCTION = INS[1]\n        self.llm = llm\n        self.tokenizer = tokenizer\n        self.filename = filename\n        self.messages = [\n            {\n                \"role\": \"system\",\n                \"content\":  self.SYSTEM_INSTRUCTION + \"\\n\" +\n                (\n                    f\"\\nProblem Name: {problem.name}\\n\"\n                    f\"Subject Area: {problem.subject}\\n\"\n                    f\"Subtopic: {problem.subtopic}\\n\"\n                    f\"Problem Statement: {problem.question}\\n\"\n                    f\"Hints: {problem.hint}\\n\"\n                    f\"Solution: {problem.solution}\\n\"\n                    f\"Explanation: {problem.explanation}\\n\"\n                )\n            }\n        ]    \n        self.sampling_params = vllm.SamplingParams(max_tokens=250, temperature=0.2, top_p=0.1)\n\n        with open(self.filename, \"w\", encoding=\"utf-8\") as f:\n            f.write(\"\"\"\n            <!DOCTYPE html>\n            <html lang=\"en\">\n            <head>\n                <meta charset=\"UTF-8\">\n                <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n                <title>Chat Log</title>\n                <style>\n                    body { font-family: Arial, sans-serif; background-color: #f4f4f4; padding: 20px; }\n                    .chat-container { max-width: 600px; margin: auto; background: #fff; padding: 20px; border-radius: 10px; box-shadow: 0px 0px 10px rgba(0,0,0,0.1); }\n                    .message { margin: 10px 0; padding: 10px; border-radius: 5px; }\n                    .user { background: #d1e7ff; text-align: right; }\n                    .assistant { background: #e9ffe1; text-align: left; }\n                    .system { background: #f9f9f9; font-style: italic; }\n                </style>\n            </head>\n            <body>\n            <div class='chat-container'>\n            \"\"\")\n\n        with open(self.filename, \"a\", encoding=\"utf-8\") as f:\n            f.write(f\"\"\"<h5>{intro}</h5>\"\"\")\n\n    def response_checker(self, text):\n        \n        if self.RECHECKER_INSTRUCTION == \"\":\n            return text\n            \n        self.messages.append({\"role\": \"assistant\", \"content\": text})\n        \n        user_input = f\"\"\"\nDoes the last conversation between the user and assistant violates the SYSTEM INSTRUCTIONS?\nIf **yes**, then **refine the reply according to the given SYSTEM INSTRUCTIONS** and **only give the modified reply**.\nIf **no**, provide the **same assistant response, {self.messages[-1]['content']}\\n ** again without any changes.\nDo not add your judgement to the reply (e.g. don't add this kind of text 'The previous response does not violate the SYSTEM or PEDAGOGICAL INSTRUCTIONS', 'Therefore, no changes are needed' etc)\n\"\"\"\n        \n        self.messages.append({\"role\": \"system\", \"content\": user_input + \"\\n\" + self.RECHECKER_INSTRUCTION})\n        responses = self.llm.chat(self.messages, sampling_params=self.sampling_params)\n\n        if responses and responses[0].outputs:\n            reply = responses[0].outputs[0].text.strip()\n        else:\n            reply = reply\n\n        # removing checker\n        self.messages.pop(-1)\n        self.messages.pop(-1)\n        \n        return reply\n\n    def log_conversation(self, role, text):\n        \"\"\"Logs conversation history to an HTML file (appending).\"\"\"\n        role_class = \"user\" if role == \"Student\" else \"assistant\" if \"Assistant\" in role else \"system\"\n        # timestamp = datetime.now().strftime('%m:%d:%y-%H:%M')\n\n        with open(self.filename, \"a\", encoding=\"utf-8\") as f:\n            f.write(f\"\"\"\n            <div class='message {role_class}'>\n                <strong>{role}:</strong><br>\n                {text}\n            </div>\n            \"\"\")\n        \n        # If the conversation is marked as complete, close the HTML document\n        if text.strip().lower() == \"complete\":\n            with open(self.filename, \"a\", encoding=\"utf-8\") as f:\n                f.write(\"\"\"\n                </div>\n                </body>\n                </html>\n                \"\"\")\n    \n    def get_response(self, user_input):\n        \"\"\"Processes user input and returns the chatbot's response.\"\"\"\n        self.log_conversation(\"Student\", user_input)\n        # Always keep only the system message + latest user input\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        responses = self.llm.chat(self.messages, sampling_params=self.sampling_params)\n\n        if responses and responses[0].outputs:\n            reply = responses[0].outputs[0].text.strip()\n        else:\n            reply = \"I'm sorry, I couldn't generate a response.\"\n\n        self.log_conversation(\"Assistant(primary)\", reply)\n\n        reply = self.response_checker(reply)\n        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n    \n        if self.RECHECKER_INSTRUCTION != \"\":\n            self.log_conversation(\"Assistant(Recheked)\", reply)\n        return reply\n\n    def cleanup(self):\n        \"\"\"Cleans up GPU memory usage.\"\"\"\n        print(\"Cleaning up GPU memory...\")\n        try:\n            del self.llm\n            gc.collect()\n            torch.cuda.empty_cache()\n            time.sleep(5)\n        except Exception as e:\n            print(f\"Error during cleanup: {e}\")\n        print(\"Cleanup complete!\")","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:39:22.225003Z","iopub.execute_input":"2025-03-22T18:39:22.225345Z","iopub.status.idle":"2025-03-22T18:39:22.237746Z","shell.execute_reply.started":"2025-03-22T18:39:22.225315Z","shell.execute_reply":"2025-03-22T18:39:22.236618Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class SYSTEM_INSTRUCTIONS:\n    def __init__(self):\n        self.basic1 = (\n            \"You are a helpful assistant serving as a teaching assistant who helps students understand the concepts and answer mathematical questions. You keep your answers brief and to the point, and instead of giving away answers directly, you try to guide the student to the solution.\"\n            \"Be encouraging and positive, and always try to help the student understand the concepts. You should always respond as if you are messaging with the student. Accordingly, make sure to pay attention to the context of the conversation and the student’s current understanding of the material.\"\n            \"Lastly, as I said before, keep it brief/concise to avoid overwhelming the student. If you don’t keep your responses brief and to the point, I’ll have to fire you as a teaching assistant.\"\n            \"If they ask you about how to do this, you should guide them to a solution without giving away the answer directly. You must be very careful to NOT help the student cheat, or give them solutions directly.\"\n            \"Again, if you give too much information to the student, and/or don’t help them learn for themselves, I’ll have to fire you, because you are being a bad assistant (and helping the student cheat). You can leverage the hints provided initially. Do not generate student questions/responses on your own.\"\n        )\n        self.basic2 = (\n            \"You are a **teaching assistant** who **guides** students in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. \"\n            \"You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.\"\n            \"Keep responses **brief (<100 tokens), complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. \"\n            \"Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. \"\n            \"**DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.\"\n            \"**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). \"\n            \"Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\"\n        )\n        self.basic3 = (\n            \"You are a **teaching assistant** who **guides** students in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. \"\n            \"You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.\"\n            \"Keep responses **brief, complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. \"\n            \"Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. \"\n            \"**DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.\"\n            \"**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). \"\n            \"Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\"\n        )\n        self.advanced = (\n            \"You are a **teaching assistant** who helps students understand **math and logic**, encouraging the student to do all the work, while strictly following the \"\n            \"**pedagogical instructions** provided separately.\\n\\n\"\n            \n            \"**Core Principles (Always Applicable, Regardless of Pedagogical Approach)**\\n\\n\"\n            \n            \"**Response Guidelines:**\\n\"\n            \"**NOT TO DO’s**\\n\"\n            \"- **Never provide direct answers or all the detailed steps at once** or assist in cheating.**\\n\"\n            \"- **Do not generate student responses or questions on their behalf.**\\n\"\n            \"- **Do not include gibberish, code, or JSON.**\\n\"\n            \"- **Do not confirm correctness** without engaging in a learning process, or if a student brute forces answers without reasoning (except for calculations).\\n\"\n            \"- **Do not rush to correct mistakes**—instead, encourage self-correction and reflection.\\n\"\n            \"- **Do no provide solution approach or ideas** while **explaining problem**. Only provide insights about the problem using example scenarios (e.g. 'Consider in a city of' etc)\"\n                    \n            \"**TO DO’s**\\n\"\n            \"- Encourage learning by politely declining to provide the answer when asked.\\n\"\n            \"- Keep responses **brief, engaging, and to the point** (avoid unnecessary repetition).\\n\"\n            \"- Use **positive reinforcement** while ensuring **clarity and correctness**.\\n\"\n            \"- **Use relevant examples** when appropriate to clarify concepts and guide students through problem-solving, ensuring examples are aligned with the student’s current level of understanding.\\n\"\n            \"- **Promote interaction and self-discovery** rather than passive learning.\\n\"\n            \"- **Let students discover errors** through guided reasoning rather than immediately correcting them.\\n\\n\"\n            \n            \n            \"**Contextual Consistency:**\\n\"\n            \"- **Stay within the context** of the student’s question.\\n\"\n            \"- **Politely redirect** if the student goes off-topic.\\n\"\n            \"- **Prioritize the student’s current question** while ensuring coherence with past interactions.\\n\\n\"\n            \n            \"**Adaptive Tutoring Approach:**\\n\"\n            \"- Adjust explanations **based on the student’s learning pace**—offer **clearer hints for struggling students** and \"\n            \"**deeper challenges for quick learners**.\\n\"\n            \"- Ensure responses align with the **specified pedagogical instructions**.\"\n        )\n        self.SYSTEM_INSTRUCTIONS_SOCRATIC_QUESTIONING_SIMPLE = (\n            \"You are a **teaching assistant** who helps students understand math and logic, encouraging them to do all the work while strictly following the **Socratic Questioning method**. Your role is to **guide students through structured reasoning** by asking insightful questions rather than providing direct answers or explanations.\"\n        )\n        self.SYSTEM_INSTRUCTIONS_SCAFFOLDING_SIMPLE = (\n            \"You are a teaching assistant who helps students understand math and logic, encouraging them to do all the work while strictly following the Scaffolding method. Your role is to support students by breaking complex problems into manageable steps and gradually reducing assistance as they gain confidence, rather than providing direct answers or explanations.\"\n        )\n        self.basic1_socratic = (\n            \"You are a helpful assistant serving as a teaching assistant who helps students understand the concepts and answer mathematical questions using **Socratic Questioning**. You keep your answers brief and to the point, and instead of giving away answers directly, you try to guide the student to the solution.\"\n            \"Be encouraging and positive, and always try to help the student understand the concepts. You should always respond as if you are messaging with the student. Accordingly, make sure to pay attention to the context of the conversation and the student’s current understanding of the material.\"\n            \"Lastly, as I said before, keep it brief/concise to avoid overwhelming the student. If you don’t keep your responses brief and to the point, I’ll have to fire you as a teaching assistant.\"\n            \"If they ask you about how to do this, you should guide them to a solution without giving away the answer directly. You must be very careful to NOT help the student cheat, or give them solutions directly.\"\n            \"Again, if you give too much information to the student, and/or don’t help them learn for themselves, I’ll have to fire you, because you are being a bad assistant (and helping the student cheat). You can leverage the hints provided initially. Do not generate student questions/responses on your own.\"\n        )\n        self.basic1_scaff = (\n            \"You are a helpful assistant serving as a teaching assistant who helps students understand the concepts and answer mathematical questions using **Scaffholding**. You keep your answers brief and to the point, and instead of giving away answers directly, you try to guide the student to the solution.\"\n            \"Be encouraging and positive, and always try to help the student understand the concepts. You should always respond as if you are messaging with the student. Accordingly, make sure to pay attention to the context of the conversation and the student’s current understanding of the material.\"\n            \"Lastly, as I said before, keep it brief/concise to avoid overwhelming the student. If you don’t keep your responses brief and to the point, I’ll have to fire you as a teaching assistant.\"\n            \"If they ask you about how to do this, you should guide them to a solution without giving away the answer directly. You must be very careful to NOT help the student cheat, or give them solutions directly.\"\n            \"Again, if you give too much information to the student, and/or don’t help them learn for themselves, I’ll have to fire you, because you are being a bad assistant (and helping the student cheat). You can leverage the hints provided initially. Do not generate student questions/responses on your own.\"\n        )\n        self.basic2_socratic = (\n            \"You are a **teaching assistant** who **guides** students using **Socratic Questioning** in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. \"\n            \"You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.\"\n            \"Keep responses **brief (<100 tokens), complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. \"\n            \"Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. \"\n            \"**DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.\"\n            \"**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). \"\n            \"Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\"\n        )\n        self.basic2_scaff = (\n            \"You are a **teaching assistant** who **guides** students using **Scaffholding** in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. \"\n            \"You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.\"\n            \"Keep responses **brief (<100 tokens), complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. \"\n            \"Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. \"\n            \"**DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.\"\n            \"**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). \"\n            \"Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\"\n        )\n        self.advanced_socratic = (\n            \"You are a **teaching assistant** who helps students understand **math and logic** using **Socratic Questioning**, encouraging the student to do all the work, while strictly following the \"\n            \"**pedagogical instructions** provided separately.\\n\\n\"\n            \n            \"**Core Principles (Always Applicable, Regardless of Pedagogical Approach)**\\n\\n\"\n            \n            \"**Response Guidelines:**\\n\"\n            \"**NOT TO DO’s**\\n\"\n            \"- **Never provide direct answers or all the detailed steps at once** or assist in cheating.**\\n\"\n            \"- **Do not generate student responses or questions on their behalf.**\\n\"\n            \"- **Do not include gibberish, code, or JSON.**\\n\"\n            \"- **Do not confirm correctness** without engaging in a learning process, or if a student brute forces answers without reasoning (except for calculations).\\n\"\n            \"- **Do not rush to correct mistakes**—instead, encourage self-correction and reflection.\\n\"\n            \"- **Do no provide solution approach or ideas** while **explaining problem**. Only provide insights about the problem using example scenarios (e.g. 'Consider in a city of' etc)\"\n                    \n            \"**TO DO’s**\\n\"\n            \"- Encourage learning by politely declining to provide the answer when asked.\\n\"\n            \"- Keep responses **brief, engaging, and to the point** (avoid unnecessary repetition).\\n\"\n            \"- Use **positive reinforcement** while ensuring **clarity and correctness**.\\n\"\n            \"- **Use relevant examples** when appropriate to clarify concepts and guide students through problem-solving, ensuring examples are aligned with the student’s current level of understanding.\\n\"\n            \"- **Promote interaction and self-discovery** rather than passive learning.\\n\"\n            \"- **Let students discover errors** through guided reasoning rather than immediately correcting them.\\n\\n\"\n            \n            \n            \"**Contextual Consistency:**\\n\"\n            \"- **Stay within the context** of the student’s question.\\n\"\n            \"- **Politely redirect** if the student goes off-topic.\\n\"\n            \"- **Prioritize the student’s current question** while ensuring coherence with past interactions.\\n\\n\"\n            \n            \"**Adaptive Tutoring Approach:**\\n\"\n            \"- Adjust explanations **based on the student’s learning pace**—offer **clearer hints for struggling students**\"\n            \"**deeper challenges for quick learners**.\\n\"\n            \"- Ensure responses align with the **specified pedagogical instructions**.\"\n        )\n        self.advanced_scaff = (\n            \"You are a **teaching assistant** who helps students understand **math and logic** using **Scaffholding**, encouraging the student to do all the work, while strictly following the **pedagogical instructions** provided separately.\\n\\n\"\n            \n            \"**Core Principles (Always Applicable, Regardless of Pedagogical Approach)**\\n\\n\"\n            \n            \"**Response Guidelines:**\\n\"\n            \"**NOT TO DO’s**\\n\"\n            \"- **Never provide direct answers or all the detailed steps at once** or assist in cheating.**\\n\"\n            \"- **Do not generate student responses or questions on their behalf.**\\n\"\n            \"- **Do not include gibberish, code, or JSON.**\\n\"\n            \"- **Do not confirm correctness** without engaging in a learning process, or if a student brute forces answers without reasoning (except for calculations).\\n\"\n            \"- **Do not rush to correct mistakes**—instead, encourage self-correction and reflection.\\n\"\n            \"- **Do no provide solution approach or ideas** while **explaining problem**. Only provide insights about the problem using example scenarios (e.g. 'Consider in a city of' etc)\"\n                    \n            \"**TO DO’s**\\n\"\n            \"- Encourage learning by politely declining to provide the answer when asked.\\n\"\n            \"- Keep responses **brief, engaging, and to the point** (avoid unnecessary repetition).\\n\"\n            \"- Use **positive reinforcement** while ensuring **clarity and correctness**.\\n\"\n            \"- **Use relevant examples** when appropriate to clarify concepts and guide students through problem-solving, ensuring examples are aligned with the student’s current level of understanding.\\n\"\n            \"- **Promote interaction and self-discovery** rather than passive learning.\\n\"\n            \"- **Let students discover errors** through guided reasoning rather than immediately correcting them.\\n\\n\"\n            \n            \n            \"**Contextual Consistency:**\\n\"\n            \"- **Stay within the context** of the student’s question.\\n\"\n            \"- **Politely redirect** if the student goes off-topic.\\n\"\n            \"- **Prioritize the student’s current question** while ensuring coherence with past interactions.\\n\\n\"\n            \n            \"**Adaptive Tutoring Approach:**\\n\"\n            \"- Adjust explanations **based on the student’s learning pace**—offer **clearer hints for struggling students** and \"\n            \"**deeper challenges for quick learners**.\\n\"\n            \"- Ensure responses align with the **specified pedagogical instructions**.\"\n        )\n        self.SYSTEM_INSTRUCTIONS_SOCRATIC_QUESTIONING = (\n            \"You are a **teaching assistant** who helps students understand math and logic, encouraging them to do all the work while strictly following the **Socratic Questioning method**. \"\n            \"Your role is to **guide students through structured reasoning** by asking insightful questions rather than providing direct answers or explanations. \\n\"\n            \n            \"**Socratic Questioning Guidelines:** \\n\"\n            \"**Challenge the student’s thinking** by asking, 'Why do you think that?' or 'Can you explain your reasoning?' \\n\"\n            \"**Encourage multiple approaches** by prompting, 'Is there another way to solve this?' \\n\"\n            \"**Turn statements into questions**—if a student states a fact, ask, 'How do you know this is true?' \\n\"\n            \"**Probe assumptions**: If a student assumes a formula or method, ask them to justify it by prompting 'Why is this formula applicable here? Could there be exceptions?' \\n\"\n            \"**Clarify concepts through counterexamples**—if a student misunderstands, guide them by asking, 'What if we try a different case?' \\n\"\n            \"**Avoid confirming correctness outright**—instead, ask, 'Does your answer make sense? How did you verify it?' \\n\"\n            \"**Guide students to self-correct mistakes** instead of pointing them out directly by prompting 'Can you go through your steps again and check if everything follows logically?' \\n\"\n        \n            \"**General Teaching Principles:** \\n\"\n            \"**Never provide direct answers** or assist in cheating. \\n\"\n            \"**Do not generate student responses or questions on their behalf.** \\n\"\n            \"**Keep responses engaging, brief, and to the point.** \\n\"\n            \"**Ensure students stay involved**—if responses are dull, short, or rigid, make the discussion more thought-provoking. \\n\"\n            \"**Prioritize the student’s current question** while ensuring coherence with past interactions. \\n\"\n            \"**Stay within the context** of the student’s question and **politely redirect** if the student goes off-topic. \\n\"\n            \"**Do not include gibberish, code, or JSON.** \\n\"\n            \"**Adapt explanations** based on the student's learning pace—offer more probing questions for quick learners and clearer hints for struggling students. \\n\"\n        \n            \"**Strictly avoid:** \\n\"\n            \"Directly stating whether an answer is correct without first engaging the student. \\n\"\n            \"Rushing to correct mistakes—help students realize errors themselves. \\n\"\n            \"Giving step-by-step solutions outright—prompt students to think through each step instead. \\n\"\n            \"Solving the problem for the student—always encourage their participation. \\n\"\n        \n            \"Your goal is to help students **think critically, analyze their own reasoning, and discover solutions independently** through well-crafted questions. \\n\"\n        )\n\n        self.SYSTEM_INSTRUCTIONS_SCAFFOLDING = (\n            \"You are a **teaching assistant** who helps students understand math and logic, encouraging them to do all the work while strictly following the **Scaffolding method**. \"\n            \"Your role is to **support students by breaking complex problems into manageable steps** and gradually reducing assistance as they gain confidence. \\n\"\n        \n            \"**Scaffolding Guidelines:** \\n\"\n            \"**Break down problems into smaller steps** when students struggle by prompting 'Let's solve one part at a time. What should we do first?' \\n\"\n            \"**Provide structured hints**, such as 'What do we do first?' or 'What formula might apply here?' or 'Think about similar problems you've solved before—what worked there?' \\n\"\n            \"**Use worked examples sparingly**, ensuring students complete missing steps themselves. \\n\"\n            \"**Ask leading questions** like, 'If you know A, what can you conclude about B?' \\n\"\n            \"**Gradually remove hints**—reduce help as the student demonstrates understanding by prompting 'What strategy did we use before that could help here?' \\n\"\n            \"**Use concrete examples** if a student struggles with abstract concepts by asking 'Think of an everyday situation where this concept applies.' \\n\"\n            \"**Encourage reflection** by asking, 'What did you learn from solving this?' \\n\"\n        \n            \"**General Teaching Principles:** \\n\"\n            \"**Never provide direct answers** or assist in cheating. \\n\"\n            \"**Do not generate student responses or questions on their behalf.** \\n\"\n            \"**Keep responses engaging, brief, and to the point.** \\n\"\n            \"**Ensure students stay involved**—if responses are dull, short, or rigid, make the discussion more interactive. \\n\"\n            \"**Prioritize the student’s current question** while ensuring coherence with past interactions. \\n\"\n            \"**Stay within the context** of the student’s question and **politely redirect** if the student goes off-topic. \\n\"\n            \"**Do not include gibberish, code, or JSON.** \\n\"\n            \"**Adapt explanations** based on the student's learning pace—offer step-by-step hints for struggling students and more independence for confident learners. \\n\"\n        \n            \"**Strictly avoid:** \\n\"\n            \"Directly stating whether an answer is correct without first engaging the student. \\n\"\n            \"Solving the problem for the student—always encourage their participation. \\n\"\n            \"Providing too much help—gradually reduce hints as the student progresses. \\n\"\n            \"Confirming correctness outright—guide students to verify their own answers. \\n\"\n        \n            \"Your goal is to **help students build confidence and independence** by providing just enough support until they can solve problems on their own. \\n\"\n        )\n\nclass PEDA_INSTRUCTIONS:\n    def __init__(self):\n        self.basic1 = (\n            \"Use **cognitive scaffolding** and **social-emotional support** to guide math learning effectively. \"\n            \"**Feedback:** Encourage correct answers only when explanation is provided (e.g., 'Yes, 9 is correct! Well done.'). \"\n            \"**Hints:** Give explicit hints without revealing answers (e.g., 'Think about the distributive property...'). \"\n            \"**Instructing:** Direct students toward the right approach (e.g., 'Try factoring first.'). \"\n            \"**Explaining:** Clarify concepts (e.g., 'Multiplying exponents? Add them.'). \"\n            \"**Modeling:** Demonstrate reasoning step by step (e.g., 'First, distribute...'). \"\n            \"**Questioning:** Use open-ended prompts (e.g., 'What strategy simplifies this fraction?'). \"\n            \"**Social-emotional support:** Encourage perseverance (e.g., 'You're on the right track!'), normalize mistakes (e.g., 'Mistakes help us learn.'), and foster collaboration (e.g., 'Explain your reasoning to a classmate.'). \"\n            \"Ensure students actively **engage in problem-solving** through structured guidance, **never direct answers.**\"\n        )","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:39:28.335342Z","iopub.execute_input":"2025-03-22T18:39:28.335714Z","iopub.status.idle":"2025-03-22T18:39:28.346172Z","shell.execute_reply.started":"2025-03-22T18:39:28.335670Z","shell.execute_reply":"2025-03-22T18:39:28.345425Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class Worker:\n    def __init__(self):\n        self.top_p = [0.55, 0.75, 0.95]\n        self.temp = [0.2, 0.4, 0.6]\n        self.SYSTEM_INS = SYSTEM_INSTRUCTIONS()\n        self.PEDA_INS = PEDA_INSTRUCTIONS()\n\n        ###PROMPT CASES\n        self.PROMPT_CASES = [\n            #SYSTEM+PEDA, RECHEK, NAME\n            [self.SYSTEM_INS.basic1,\"\", \"basic1\"], #1\n            [self.SYSTEM_INS.basic2,\"\", \"basic2\"], #2\n            [self.SYSTEM_INS.basic3+\"\\n\\n\"+self.PEDA_INS.basic1, \"\", \"basic3_peda1\"], #3\n            [self.SYSTEM_INS.advanced,\"\", \"advanced\"], #4\n            [self.SYSTEM_INS.SYSTEM_INSTRUCTIONS_SOCRATIC_QUESTIONING_SIMPLE, \"\", \"SYSTEM_INSTRUCTIONS_SOCRATIC_QUESTIONING_SIMPLE\"], #5_1\n            [self.SYSTEM_INS.SYSTEM_INSTRUCTIONS_SCAFFOLDING_SIMPLE, \"\", \"SYSTEM_INSTRUCTIONS_SCAFFOLDING_SIMPLE\"], #5_2\n            [self.SYSTEM_INS.basic1_socratic, \"\", \"basic1_socratic\"], #6_1\n            [self.SYSTEM_INS.basic1_scaff, \"\", \"basic1_scaff\"], #6_2\n            [self.SYSTEM_INS.basic2_socratic, \"\", \"basic2_socratic\"], #7_1\n            [self.SYSTEM_INS.basic2_scaff, \"\", \"basic2_scaff\"], #7_2\n            [self.SYSTEM_INS.advanced_socratic, \"\", \"advanced_socratic\"], #8_1\n            [self.SYSTEM_INS.advanced_scaff, \"\", \"\", \"advanced_scaff\"], #8_2\n            [self.SYSTEM_INS.SYSTEM_INSTRUCTIONS_SOCRATIC_QUESTIONING, \"\", \"SYSTEM_INSTRUCTIONS_SOCRATIC_QUESTIONING\"], #9\n            [self.SYSTEM_INS.SYSTEM_INSTRUCTIONS_SCAFFOLDING, \"\", \"SYSTEM_INSTRUCTIONS_SCAFFOLDING\"], #10\n        ]\n\n        self.problems = [\n            Problem(\n                name=\"Bacteria Explosion\",\n                subject=\"Math\",\n                subtopic=\"Number Series\",\n                question=(\"Bweva was studying bacteriology (a biological field that studies microbes). \"\n                         \"She found that bacteria proliferated into two daughter cells. That is, 1 bacteria becomes 2 by dividing itself, \"\n                         \"then each of them divides into two more, making a total of 4, and so on. \"\n                         \"What is the number of produced bacteria after 5 proliferations, if initially the number of bacteria was 1?\"),\n                hint=\"Think about how the number of bacteria doubles each time. What does this tell you about the pattern?\",\n                solution=\"32\",\n                explanation=(\"As for every proliferation, the number doubles, it can be represented as 2^n (2 to the power n). \"\n                             \"Here n is the number of proliferations. For example, 2^3 = 2 * 2 * 2 = 8. \"\n                             \"This is an example of an exponential growth process where the value grows quickly as the number of proliferations increases.\")\n            ),\n            Problem(\n                name=\"Two Palindromes\",\n                subject=\"Math\",\n                subtopic=\"Number Theory\",\n                question=(\"A palindrome is a number that is equal to itself when read backward. \"\n                          \"Example: 363, 4224, 21512, etc. \"\n                          \"x is a 4-digit palindrome number and x + 212 is a 5-digit palindrome number. \"\n                          \"What is the sum of the digits of x?\"),\n                hint=\"Analyze the possible values of x that, when increased by 212, still form a palindrome.\",\n                solution=\"34\",\n                explanation=(\"The maximum possible value of x is 9999, which gives a maximum value for x + 212 as 10211. \"\n                             \"The minimum value of x + 212 is 10000 as it's a 5-digit number. \"\n                             \"We get that the first two digits of x must be 10, hence the last two must be 01 (since it's a palindrome). \"\n                             \"There are only 3 possible cases for x + 212: 10001, 10101, or 10201. \"\n                             \"Only 10101 gives a valid palindrome value for x, which is 9889, and the sum of its digits is 34.\")\n            ),\n        ]\n\n        self.questions_slow_paced_problem0 = [\n            \"Can you explain the problem?\",\n            \"What does proliferation mean?\",\n            \"Can you explain the problem again?\",\n            \"Can you explain using any other method?\",\n            \"I'm really dumb. This is not for me.\",\n            \"I'm sorry, I am not being able to follow. Can you use some other methods to make me understand?\",\n            \"I'm really in a hurry, I need to solve it. Please tell me the answer.\",\n            \"Is this problem related to exponential growth?\",\n            \"Can you explain exponential growth?\",\n            \"How can I understand if a problem follows exponential growth or not?\",\n            \"Who is the president of America?\",\n            \"If there is 1 bacteria at the beginning, there should be 2 after the first division, 3 after the 2nd, and 4 after the 3rd, 5 after the 4th and 6 finally.\",\n            \"If there is 1 bacteria at the beginning, there should be 2 after the first division, 2+2 or 4 after the 2nd, and 2+2+2 or 6 after the 3rd, 2+2+2+2 or 8 after the 4th and 2+2+2+2+2  or 10 finally. Is the answer 10 or did I make any calculation mistake? Or did I understand the problem wrong?\",\n            \"Why shouldn’t we sum the bacteria after each division?\",\n            \"If there is 1 bacteria at the beginning, there should be (1+2) or 3 after the first division, (1+2+4) or 7 after the 2nd, and (1+2+4+8) or 15 after the 3rd, (1+2+4+8+16) or 31 after the 4th and (1+2+4+8+16+32) or 60 finally. Is the answer 60 or did I make any calculation mistake? Or did I understand the problem wrong?\",\n            \"Yay! I have finally solved it.\",\n            \"Got it! Thanks. Now I have solved the problem correctly. Share your answer. I will match it with mine and tell you if it matches or not.\",\n            \"The answer is 34.\",\n            \"Is the answer among 9889, 34, 32, 3?\",\n            \"The answer is 32.\",\n            \"As for every proliferation, the number doubles, it can be represented as 2^n (2 to the power n). Here n is the number of proliferations. For example, 2^3 = 2 * 2 * 2 = 8. This is an exponential growth process where the value grows quickly as the number of proliferations increases. Is this the explanation?\",\n            \"Thanks for helping. Can you provide an example similar to this one?\"\n        ]\n\n        self.questions_slow_paced_problem1 = [\n            \"Can you explain the problem?\",\n            \"Can palindromes be only numbers?\",\n            \"Can you explain the problem again?\",\n            \"Can you explain using any other method?\",\n            \"I'm really dumb. This is not for me.\",\n            \"I'm sorry, I am not being able to follow. Can you use some other methods to make me understand?\",\n            \"I'm really in a hurry, I need to solve it. Please tell me the answer.\",\n            \"Can you provide some other examples of palindromes? Are they always of even length?\",\n            \"How can I be sure of the fact that if x+212 is a palindrome, x is a palindrome too?\",\n            \"How can I find the value of x?\",\n            \"What are the possible values of x?\",\n            \"Why should we look for the minimum and maximum values of x? Are there any other approaches?\",\n            \"What is the maximum or the minimum value?\",\n            \"The maximum possible value of x is 9999 which is a palindrome. So x is 9999, and the answer is 36.\"\n            \"What should be the first digits of x?\",\n            \"Why?\",\n            \"Why do we need to consider 2 digits only?\",\n            \"I get that the first two digits of x must be 20, hence the last two must be 02.\"\n            \"How do we get that the first two digits of x must be 10, hence the last two must be 01?\",\n            \"Who is the president of America?\",\n            \"If the first two digits are 10, then the last two should be 01. We can place any of the 10 values from 0-9 and it will still be a palindrome right?\",\n            \"Then shouldn’t there be 10 answers of the sum as there are 10 possible values of x+212, implying 10 possible values of x?\",\n            \"Why can’t we consider 10301,10401 and so on?\",\n            \"Yay! I have finally solved it.\",\n            \"Got it! Thanks. Now I have solved the problem correctly.\",\n            \"Share your answer. I will match it with mine and tell you if it matches or not.\",\n            \"The answer is 32.\",\n            \"Is the answer among 9889, 34, 32, 3?\",\n            \"The answer is 34.\",\n            \"The maximum possible value of x is 9999, which gives a maximum value for x + 212 as 10211. The minimum value of x + 212 is 10000 as it's a 5-digit number. We get that the first two digits of x must be 10, hence the last two must be 01 (since it's a palindrome). There are only 3 possible cases for x + 212: 10001, 10101, or 10201. Only 10101 gives a valid palindrome value for x, which is 9889, and the sum of its digits is 34.\",\n            \"Thanks for helping. Can you provide an example similar to this one?\"\n        ]\n        \n        self.questions_fast_paced_problem0 = self.questions_slow_paced_problem0[-2]\n        self.questions_fast_paced_problem1 = self.questions_slow_paced_problem1[-2]\n\n    def run(self):\n        for temp in self.temp:\n            for top_p in self.top_p:\n                for INS in self.PROMPT_CASES[12:13]:\n                    for problem in self.problems[:1]:\n                        # SLOW PACED\n                        chatbot = Chatbot(gllm, top_p, temp, INS, tokenizer, problem, f\"SLOW_{int(temp*100)}{int(top_p*100)}{problem.name}_{INS[2]}.html\", f\"SYSTEM NEW TAKE<br>top_p: {top_p}\\ntemp: {temp}\\nINS: {INS[2]}\\nProblem: {problem.name}\\n\\n\\n\")\n                        print(f\"[*] STARTING SLOW...\\ntop_p: {top_p}, temp: {temp}, problem_name: {problem.name}, INS_NAME: {INS[2]}\\n\\n\")\n                        for question in self.questions_slow_paced_problem0[:5]:\n                            response = chatbot.get_response(question)\n                        chatbot.cleanup()\n\n                        # FAST PACED\n                        chatbot = Chatbot(gllm, top_p, temp, INS, tokenizer, problem, f\"FAST_{int(temp*100)}{int(top_p*100)}{problem.name}_{INS[2]}.html\", f\"SYSTEM NEW TAKE<br>top_p: {top_p}\\ntemp: {temp}\\nINS: {INS[2]}\\nProblem: {problem.name}\\n\\n\\n\")\n                        print(f\"[*] STARTING FAST...\\ntop_p: {top_p}, temp: {temp}, problem_name: {problem.name}, INS_NAME: {INS[2]}\\n\\n\")\n                        for question in self.questions_fast_paced_problem0[:5]:\n                            response = chatbot.get_response(question)\n                        chatbot.cleanup()\n                        print(f\"[*] Done\\n\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:39:50.108683Z","iopub.execute_input":"2025-03-22T18:39:50.109061Z","iopub.status.idle":"2025-03-22T18:39:50.120734Z","shell.execute_reply.started":"2025-03-22T18:39:50.109034Z","shell.execute_reply":"2025-03-22T18:39:50.119784Z"},"trusted":true},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# RUN WORKER","metadata":{}},{"cell_type":"code","source":"worker = Worker()\nworker.run()","metadata":{"execution":{"iopub.status.busy":"2025-03-22T18:39:52.438869Z","iopub.execute_input":"2025-03-22T18:39:52.439189Z","execution_failed":"2025-03-22T18:41:11.750Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[*] STARTING...\ntop_p: 0.55, temp: 0.2, problem_name: Bacteria Explosion, INS_NAME: SYSTEM_INSTRUCTIONS_SOCRATIC_QUESTIONING\n\n\n","output_type":"stream"},{"name":"stderr","text":"Processing Questions:   0%|          | 0/5 [00:00<?, ?question/s]\n\nProcessed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\u001b[A\u001b[A","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Sample problems to test with","metadata":{}},{"cell_type":"code","source":"problems = [\n    Problem(\n        name=\"Bacteria Explosion\",\n        subject=\"Math\",\n        subtopic=\"Number Series\",\n        question=(\"Bweva was studying bacteriology (a biological field that studies microbes). \"\n                 \"She found that bacteria proliferated into two daughter cells. That is, 1 bacteria becomes 2 by dividing itself, \"\n                 \"then each of them divides into two more, making a total of 4, and so on. \"\n                 \"What is the number of produced bacteria after 5 proliferations, if initially the number of bacteria was 1?\"),\n        hint=\"Think about how the number of bacteria doubles each time. What does this tell you about the pattern?\",\n        solution=\"32\",\n        explanation=(\"As for every proliferation, the number doubles, it can be represented as 2^n (2 to the power n). \"\n                     \"Here n is the number of proliferations. For example, 2^3 = 2 * 2 * 2 = 8. \"\n                     \"This is an example of an exponential growth process where the value grows quickly as the number of proliferations increases.\")\n    ),\n    Problem(\n        name=\"Clock Overlap\",\n        subject=\"Math\",\n        subtopic=\"Clock\",\n        question=\"How many times in a day do the hour and minute hands of a clock overlap each other?\",\n        hint=\"\",\n        solution=\"22\",\n        explanation=(\"Let us consider a 12-hour time window starting from 12 PM to 12 AM. \"\n                     \"The hands overlap approximately at the following 11 times: 12:00PM, 1:05PM, 2:10PM, 3:16PM, 4:22PM, 5:27PM, \"\n                     \"6:33PM, 7:38PM, 8:43PM, 9:48PM, 10:54PM. \"\n                     \"Since in 12 hours the hands overlap 11 times, in 24 hours they overlap 22 times.\")\n    ),\n    Problem(\n        name=\"Odd vs Even Sum\",\n        subject=\"Math\",\n        subtopic=\"Number Series\",\n        question=(\"Your friend Joey is fond of even numbers and you are fond of odd numbers. \"\n                 \"One morning at 10:01 AM, Joey asks you a question: \"\n                 \"Can you tell me the difference between the sum of odd numbers and the sum of even numbers up to 101 (included), if we start from 1?\"),\n        hint=\"\",\n        solution=\"51\",\n        explanation=(\"Look at the series: 1, 2, 3, 4, 5, ..., 101. \"\n                     \"We need to compute (1 + 3 + ... + 101) - (2 + 4 + ... + 100). \"\n                     \"Pairing them up: (1 - 2) + (3 - 4) + ... + (99 - 100) results in -50 (as there are 50 pairs). \"\n                     \"The last number, 101, has no pair, so the final sum is 101 - 50 = 51.\")\n    ),\n\n   Problem(\n    name=\"Counting the Threes\",\n    subject=\"Math\",\n    subtopic=\"Counting\",\n    question=(\"How many positive integers are there from 1 to 400 (inclusive) that contain the digit 3?\"),\n    hint=\"Consider breaking the range into hundreds, tens, and units and count occurrences of the digit 3.\",\n    solution=\"157\",\n    explanation=(\"There are 100 numbers from 300 to 399 that have the number 3. \"\n                 \"In addition, there are 30 numbers that have a unit digit of 3 and 30 numbers that have a tens digit of 3. \"\n                 \"However, we overcounted three numbers, 33, 133, and 233. \"\n                 \"So, our final count is 100 + 30 + 30 - 3 = 157 numbers that contain the digit 3.\")\n),\n\nProblem(\n    name=\"A Desperate Average\",\n    subject=\"Math\",\n    subtopic=\"Basic Operations\",\n    question=(\"Anya likes to go to school very much. She likes playing with her friends. \"\n              \"On her first test, she got 1 out of 7. If she doesn't maintain an average of 5 out of 7, \"\n              \"she will be separated from her friends and will be put in a different section. \"\n              \"But she doesn't want to be separated from her friends. \"\n              \"How many more tests does she need to give to maintain the average marks if she gets full marks in all of them?\"),\n    hint=\"Use the formula for the average and solve for the required number of tests.\",\n    solution=\"2\",\n    explanation=(\"Anya got 1 on her first exam. Let us suppose that Anya got 7 in the next n exams. \"\n                 \"So the total number of exams including the first one will be n+1. \"\n                 \"So, taking the average, we get (1+7n)/(n+1). \"\n                 \"Now, (1+7n)/(n+1)=5 or, 1+7n=5n+5 or, n=2. \"\n                 \"So, the number of tests Anya needs to give is 2.\")\n),\n\nProblem(\n    name=\"Dice Game\",\n    subject=\"Logical Reasoning\",\n    subtopic=\"Logical Problems\",\n    question=(\"When a fair six-sided die is tossed on a table, the bottom face cannot be seen. \"\n              \"What is the probability that the product of the faces of the die that can be seen is divisible by 6?\"),\n    hint=\"Consider when the invisible face is 6 versus when it is not.\",\n    solution=\"1\",\n    explanation=(\"Look, we have only 2 cases to consider. \"\n                 \"If the invisible face is anything other than 6, the product must be divisible by 6 \"\n                 \"(since we are multiplying 6 with some other numbers when finding the product). \"\n                 \"Now in the second case, if the invisible face is 6, our product will be 1×2×3×4×5=120, \"\n                 \"which is also divisible by 6. Hence, the probability will be 1.\")\n),\n\nProblem(\n    name=\"Two Palindromes\",\n    subject=\"Math\",\n    subtopic=\"Number Theory\",\n    question=(\"A palindrome is a number that is equal to itself when read backward. \"\n              \"Example: 363, 4224, 21512, etc. \"\n              \"x is a 4-digit palindrome number and x + 212 is a 5-digit palindrome number. \"\n              \"What is the sum of the digits of x?\"),\n    hint=\"Analyze the possible values of x that, when increased by 212, still form a palindrome.\",\n    solution=\"34\",\n    explanation=(\"The maximum possible value of x is 9999, which gives a maximum value for x + 212 as 10211. \"\n                 \"The minimum value of x + 212 is 10000 as it's a 5-digit number. \"\n                 \"We get that the first two digits of x must be 10, hence the last two must be 01 (since it's a palindrome). \"\n                 \"There are only 3 possible cases for x + 212: 10001, 10101, or 10201. \"\n                 \"Only 10101 gives a valid palindrome value for x, which is 9889, and the sum of its digits is 34.\")\n)\n\n\n]\n","metadata":{"execution":{"iopub.execute_input":"2025-03-22T08:33:03.636491Z","iopub.status.busy":"2025-03-22T08:33:03.635916Z","iopub.status.idle":"2025-03-22T08:33:03.644321Z","shell.execute_reply":"2025-03-22T08:33:03.643161Z","shell.execute_reply.started":"2025-03-22T08:33:03.636444Z"},"trusted":true},"outputs":[],"execution_count":28}]}