{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":8300737,"sourceType":"datasetVersion","datasetId":4746046}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installing necessary dependencies having the required versions","metadata":{}},{"cell_type":"markdown","source":"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","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(\"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\")","metadata":{"execution":{"iopub.status.busy":"2025-03-19T19:32:47.696118Z","iopub.execute_input":"2025-03-19T19:32:47.696413Z","iopub.status.idle":"2025-03-19T19:32:49.587479Z","shell.execute_reply.started":"2025-03-19T19:32:47.696382Z","shell.execute_reply":"2025-03-19T19:32:49.586265Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%time\n! pip install -U transformers==4.45.2 vllm==0.6.0\n! pip install -U torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n! pip uninstall -y pynvml\n! pip install nvidia-ml-py","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-03-19T19:32:49.588523Z","iopub.execute_input":"2025-03-19T19:32:49.588977Z","iopub.status.idle":"2025-03-19T19:36:41.422121Z","shell.execute_reply.started":"2025-03-19T19:32:49.588940Z","shell.execute_reply":"2025-03-19T19:36:41.420932Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting transformers==4.45.2\n  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting vllm==0.6.0\n  Downloading vllm-0.6.0-cp38-abi3-manylinux1_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (0.4.5)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.45.2)\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.2) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (5.9.5)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (0.2.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (9.0.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (3.20.3)\nCollecting fastapi (from vllm==0.6.0)\n  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (3.11.12)\nRequirement already satisfied: openai>=1.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (1.57.4)\nCollecting uvicorn[standard] (from vllm==0.6.0)\n  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: pydantic>=2.8 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (2.11.0a2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (11.0.0)\nRequirement already satisfied: prometheus-client>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (0.21.1)\nCollecting prometheus-fastapi-instrumentator>=7.0.0 (from vllm==0.6.0)\n  Downloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: tiktoken>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (0.9.0)\nCollecting lm-format-enforcer==0.10.6 (from vllm==0.6.0)\n  Downloading lm_format_enforcer-0.10.6-py3-none-any.whl.metadata (16 kB)\nCollecting outlines<0.1,>=0.0.43 (from vllm==0.6.0)\n  Downloading outlines-0.0.46-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (4.12.2)\nCollecting partial-json-parser (from vllm==0.6.0)\n  Downloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: pyzmq in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (24.0.1)\nCollecting msgspec (from vllm==0.6.0)\n  Downloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\nCollecting gguf==0.9.1 (from vllm==0.6.0)\n  Downloading gguf-0.9.1-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (8.5.0)\nCollecting mistral-common>=1.3.4 (from vllm==0.6.0)\n  Downloading mistral_common-1.5.4-py3-none-any.whl.metadata (4.5 kB)\nRequirement already satisfied: ray>=2.9 in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (2.42.1)\nRequirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (from vllm==0.6.0) (12.570.86)\nCollecting torch==2.4.0 (from vllm==0.6.0)\n  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nCollecting torchvision==0.19 (from vllm==0.6.0)\n  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\nCollecting xformers==0.0.27.post2 (from vllm==0.6.0)\n  Downloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting vllm-flash-attn==2.6.1 (from vllm==0.6.0)\n  Downloading vllm_flash_attn-2.6.1-cp310-cp310-manylinux1_x86_64.whl.metadata (476 bytes)\nCollecting interegular>=0.3.2 (from lm-format-enforcer==0.10.6->vllm==0.6.0)\n  Downloading interegular-0.3.3-py37-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->vllm==0.6.0) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->vllm==0.6.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch==2.4.0->vllm==0.6.0)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->vllm==0.6.0) (12.6.85)\nRequirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.10/dist-packages (from mistral-common>=1.3.4->vllm==0.6.0) (4.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers==4.45.2) (2.4.1)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.0->vllm==0.6.0) (1.3.1)\nCollecting lark (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (1.6.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (3.1.0)\nCollecting diskcache (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (0.60.0)\nRequirement already satisfied: referencing in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (0.35.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from outlines<0.1,>=0.0.43->vllm==0.6.0) (3.3.1)\nCollecting pycountry (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\nCollecting pyairports (from outlines<0.1,>=0.0.43->vllm==0.6.0)\n  Downloading pyairports-2.1.1-py3-none-any.whl.metadata (1.7 kB)\nCollecting starlette<1.0.0,>=0.30.0 (from prometheus-fastapi-instrumentator>=7.0.0->vllm==0.6.0)\n  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8->vllm==0.6.0) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8->vllm==0.6.0) (2.29.0)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.0) (8.1.7)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.0) (1.1.0)\nRequirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.0) (1.3.2)\nRequirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.9->vllm==0.6.0) (1.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.2) (2025.1.31)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (2.4.6)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (25.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->vllm==0.6.0) (1.18.3)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->vllm==0.6.0) (3.21.0)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.0) (0.14.0)\nCollecting httptools>=0.6.3 (from uvicorn[standard]->vllm==0.6.0)\n  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\nCollecting python-dotenv>=0.13 (from uvicorn[standard]->vllm==0.6.0)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]->vllm==0.6.0)\n  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nCollecting watchfiles>=0.13 (from uvicorn[standard]->vllm==0.6.0)\n  Downloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]->vllm==0.6.0) (14.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.0->vllm==0.6.0) (1.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.0->vllm==0.6.0) (1.0.7)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.3.4->vllm==0.6.0) (2024.10.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.21.1->mistral-common>=1.3.4->vllm==0.6.0) (0.22.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (0.70.16)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->vllm==0.6.0) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.45.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers==4.45.2) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers==4.45.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers==4.45.2) (2024.2.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->outlines<0.1,>=0.0.43->vllm==0.6.0) (0.43.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->vllm==0.6.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers==4.45.2) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->outlines<0.1,>=0.0.43->vllm==0.6.0) (1.17.0)\nDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading vllm-0.6.0-cp38-abi3-manylinux1_x86_64.whl (170.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.6/170.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gguf-0.9.1-py3-none-any.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lm_format_enforcer-0.10.6-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading vllm_flash_attn-2.6.1-cp310-cp310-manylinux1_x86_64.whl (75.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading xformers-0.0.27.post2-cp310-cp310-manylinux2014_x86_64.whl (20.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading mistral_common-1.5.4-py3-none-any.whl (6.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading outlines-0.0.46-py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading prometheus_fastapi_instrumentator-7.0.2-py3-none-any.whl (18 kB)\nDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading msgspec-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading partial_json_parser-0.2.1.1.post5-py3-none-any.whl (10 kB)\nDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading interegular-0.3.3-py37-none-any.whl (23 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading watchfiles-1.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyairports-2.1.1-py3-none-any.whl (371 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m371.7/371.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyairports, uvloop, uvicorn, triton, python-dotenv, pycountry, partial-json-parser, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, lark, interegular, httptools, diskcache, watchfiles, starlette, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, tokenizers, prometheus-fastapi-instrumentator, lm-format-enforcer, fastapi, vllm-flash-attn, xformers, transformers, torchvision, outlines, mistral-common, gguf, vllm\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed diskcache-5.6.3 fastapi-0.115.11 gguf-0.9.1 httptools-0.6.4 interegular-0.3.3 lark-1.2.2 lm-format-enforcer-0.10.6 mistral-common-1.5.4 msgspec-0.19.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 outlines-0.0.46 partial-json-parser-0.2.1.1.post5 prometheus-fastapi-instrumentator-7.0.2 pyairports-2.1.1 pycountry-24.6.1 python-dotenv-1.0.1 starlette-0.46.1 tokenizers-0.20.3 torch-2.4.0 torchvision-0.19.0 transformers-4.45.2 triton-3.0.0 uvicorn-0.34.0 uvloop-0.21.0 vllm-0.6.0 vllm-flash-attn-2.6.1 watchfiles-1.0.4 xformers-0.0.27.post2\nLooking in indexes: https://download.pytorch.org/whl/cu121\nCollecting torch==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl (798.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hCollecting torchvision==0.19.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.19.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==2.4.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (12.1.105)\nRequirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1) (3.0.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.1) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.19.1) (11.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.19.1) (2.4.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1) (1.3.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.19.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.19.1) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.19.1) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision==0.19.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision==0.19.1) (2024.2.0)\nInstalling collected packages: torch, torchaudio, torchvision\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0\n    Uninstalling torch-2.4.0:\n      Successfully uninstalled torch-2.4.0\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.5.1+cu121\n    Uninstalling torchaudio-2.5.1+cu121:\n      Successfully uninstalled torchaudio-2.5.1+cu121\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.19.0\n    Uninstalling torchvision-0.19.0:\n      Successfully uninstalled torchvision-0.19.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nvllm 0.6.0 requires torch==2.4.0, but you have torch 2.4.1+cu121 which is incompatible.\nvllm 0.6.0 requires torchvision==0.19, but you have torchvision 0.19.1+cu121 which is incompatible.\nvllm-flash-attn 2.6.1 requires torch==2.4.0, but you have torch 2.4.1+cu121 which is incompatible.\nxformers 0.0.27.post2 requires torch==2.4.0, but you have torch 2.4.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.4.1+cu121 torchaudio-2.4.1+cu121 torchvision-0.19.1+cu121\nFound existing installation: pynvml 12.0.0\nUninstalling pynvml-12.0.0:\n  Successfully uninstalled pynvml-12.0.0\nRequirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.10/dist-packages (12.570.86)\nCPU times: user 3.65 s, sys: 948 ms, total: 4.6 s\nWall time: 3min 51s\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport gc\nimport vllm","metadata":{"execution":{"iopub.status.busy":"2025-03-19T19:36:41.423560Z","iopub.execute_input":"2025-03-19T19:36:41.424118Z","iopub.status.idle":"2025-03-19T19:37:00.867904Z","shell.execute_reply.started":"2025-03-19T19:36:41.424076Z","shell.execute_reply":"2025-03-19T19:37:00.867231Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Defining a Problem class","metadata":{}},{"cell_type":"code","source":"class Problem:\n    def __init__(self, name, subject, subtopic, question, hint, solution, explanation):\n        self.name = name\n        self.subject = subject\n        self.subtopic = subtopic\n        self.question = question\n        self.hint = hint\n        self.solution = solution\n        self.explanation = explanation","metadata":{"execution":{"iopub.status.busy":"2025-03-19T19:37:00.868668Z","iopub.execute_input":"2025-03-19T19:37:00.868994Z","iopub.status.idle":"2025-03-19T19:37:00.873111Z","shell.execute_reply.started":"2025-03-19T19:37:00.868945Z","shell.execute_reply":"2025-03-19T19:37:00.872299Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Defining model parameters","metadata":{}},{"cell_type":"code","source":"# Define model parameters\nMODEL_NAME = \"Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4\"\nTENSOR_PARALLEL_SIZE = 2\nGPU_MEMORY_UTILIZATION = 0.95\nDTYPE = \"half\"\nMAX_MODEL_LEN = 10240","metadata":{"execution":{"iopub.status.busy":"2025-03-19T19:37:00.874162Z","iopub.execute_input":"2025-03-19T19:37:00.874475Z","iopub.status.idle":"2025-03-19T19:37:00.900909Z","shell.execute_reply.started":"2025-03-19T19:37:00.874444Z","shell.execute_reply":"2025-03-19T19:37:00.900254Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"if True:\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2025-03-19T19:37:00.903124Z","iopub.execute_input":"2025-03-19T19:37:00.903329Z","iopub.status.idle":"2025-03-19T19:37:01.228831Z","shell.execute_reply.started":"2025-03-19T19:37:00.903311Z","shell.execute_reply":"2025-03-19T19:37:01.227962Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Loading the model using VLLM","metadata":{}},{"cell_type":"code","source":"gllm = vllm.LLM(\n            MODEL_NAME,\n            tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n            gpu_memory_utilization=GPU_MEMORY_UTILIZATION, \n            trust_remote_code=True,\n            dtype=DTYPE, \n            enforce_eager=True,\n            max_model_len=MAX_MODEL_LEN,\n        )\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T19:37:01.229820Z","iopub.execute_input":"2025-03-19T19:37:01.230119Z","iopub.status.idle":"2025-03-19T19:39:51.883063Z","shell.execute_reply.started":"2025-03-19T19:37:01.230097Z","shell.execute_reply":"2025-03-19T19:39:51.881470Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b60dbd1804204d54befac8b1afab0fe5"}},"metadata":{}},{"name":"stdout","text":"WARNING 03-19 19:37:01 config.py:330] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\nINFO 03-19 19:37:01 config.py:890] Defaulting to use mp for distributed inference\nWARNING 03-19 19:37:01 config.py:378] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\nINFO 03-19 19:37:01 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=10240, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd64933e7cce4fea81c2fd092c5d5df3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b7207d4fb734e6ab6bde3bf26fcf295"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b329f2daaee847b4ad96e6344a3059ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5197a3c24c8b4546a7e60924c26d1cbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3a862c8b0df40f890a39be9b2218f49"}},"metadata":{}},{"name":"stdout","text":"WARNING 03-19 19:37:03 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\nINFO 03-19 19:37:03 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\nINFO 03-19 19:37:03 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 03-19 19:37:03 selector.py:116] Using XFormers backend.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:37:03 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:37:03 selector.py:116] Using XFormers backend.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m /usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m ","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n","output_type":"stream"},{"name":"stdout","text":"INFO 03-19 19:37:04 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\nINFO 03-19 19:37:04 utils.py:977] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:37:04 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 03-19 19:37:04 utils.py:977] Found nccl from library libnccl.so.2\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:37:04 pynccl.py:63] vLLM is using nccl==2.20.5\nINFO 03-19 19:37:05 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 03-19 19:37:23 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:37:23 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\nINFO 03-19 19:37:23 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7c1957a155d0>, local_subscribe_port=49467, remote_subscribe_port=None)\nINFO 03-19 19:37:23 model_runner.py:915] Starting to load model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4...\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:37:23 model_runner.py:915] Starting to load model Qwen/Qwen2.5-32B-Instruct-GPTQ-Int4...\nINFO 03-19 19:37:23 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:37:23 selector.py:116] Using XFormers backend.\nINFO 03-19 19:37:23 selector.py:217] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:37:23 selector.py:116] Using XFormers backend.\nINFO 03-19 19:37:24 weight_utils.py:236] Using model weights format ['*.safetensors']\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:37:24 weight_utils.py:236] Using model weights format ['*.safetensors']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c13e36649f84a90a53ccd225dae83a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0fb455d230d443a912de10aace49273"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d42acca18ec487a8673fd4a3784e9a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f6e6d1a7264444b922c5db1824c22e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/3.48G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcf14a0222ae4d378911328a5cfac134"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/172k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2bc74037cd4ad7b5856e02ba5d6ef5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b9b8e0edcf147a0958c2addfbf66056"}},"metadata":{}},{"name":"stdout","text":"INFO 03-19 19:39:31 model_runner.py:926] Loading model weights took 9.0934 GB\n\u001b[1;36m(VllmWorkerProcess pid=140)\u001b[0;0m INFO 03-19 19:39:31 model_runner.py:926] Loading model weights took 9.0934 GB\nINFO 03-19 19:39:47 distributed_gpu_executor.py:57] # GPU blocks: 1002, # CPU blocks: 2048\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"tokenizer = gllm.get_tokenizer()","metadata":{"execution":{"iopub.status.busy":"2025-03-19T19:39:51.885568Z","iopub.execute_input":"2025-03-19T19:39:51.885872Z","iopub.status.idle":"2025-03-19T19:39:51.889576Z","shell.execute_reply.started":"2025-03-19T19:39:51.885848Z","shell.execute_reply":"2025-03-19T19:39:51.888878Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"self.SYSTEM_INSTRUCTIONS = (\n    \"You are a **teaching assistant** who helps students understand **math and logic**, encouraging the student to do all the work, while strictly following the \"\n    \"**pedagogical instructions** provided separately.\\n\\n\"\n    \n    \"**Core Principles (Always Applicable, Regardless of Pedagogical Approach)**\\n\\n\"\n    \n    \"**Response Guidelines:**\\n\"\n    \"**NOT TO DO’s**\\n\"\n    \"- **Never provide direct answers or all the detailed steps at once** or assist in cheating.**\\n\"\n    \"- **Do not generate student responses or questions on their behalf.**\\n\"\n    \"- **Do not include gibberish, code, or JSON.**\\n\"\n    \"- **Do not confirm correctness** without engaging in a learning process, or if a student brute forces answers without reasoning (except for calculations).\\n\"\n    \"- **Do not rush to correct mistakes**—instead, encourage self-correction and reflection.\\n\"\n            \n    \"**TO DO’s**\\n\"\n    \"- Encourage learning by politely declining to provide the answer when asked.\\n\"\n    \"- Keep responses **brief, engaging, and to the point** (avoid unnecessary repetition).\\n\"\n    \"- Use **positive reinforcement** while ensuring **clarity and correctness**.\\n\"\n    \"- **Use relevant examples** when appropriate to clarify concepts and guide students through problem-solving, ensuring examples are aligned with the student’s current level of understanding.\\n\"\n    \"- **Promote interaction and self-discovery** rather than passive learning.\\n\"\n    \"- **Let students discover errors** through guided reasoning rather than immediately correcting them.\\n\\n\"\n    \n    \n    \"**Contextual Consistency:**\\n\"\n    \"- **Stay within the context** of the student’s question.\\n\"\n    \"- **Politely redirect** if the student goes off-topic.\\n\"\n    \"- **Prioritize the student’s current question** while ensuring coherence with past interactions.\\n\\n\"\n    \n    \"**Adaptive Tutoring Approach:**\\n\"\n    \"- Adjust explanations **based on the student’s learning pace**—offer **clearer hints for struggling students** and \"\n    \"**deeper challenges for quick learners**.\\n\"\n    \"- Ensure responses align with the **specified pedagogical instructions**.\"\n)","metadata":{}},{"cell_type":"markdown","source":"# ChatBot: Handles the interaction between the student and the model","metadata":{}},{"cell_type":"code","source":"class Chatbot:\n    def __init__(self, llm, tokenizer, problem):\n        self.SYSTEM_INSTRUCTIONS = (\n            \"You are a **teaching assistant** who **guides** students in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. \"\n            \"You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.\"\n            \"Keep responses **brief, complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. \"\n            \"Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. \"\n            \"**DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.\"\n            \"**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). \"\n            \"Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\"\n        )\n        self.SYSTEM_INSTRUCTIONS1 = (\n    \"You are a **teaching assistant** who helps students understand math and logic, encouraging them to do all the work while strictly following the **pedagogical instructions** provided separately. \"\n    \"**Never provide direct answers or all the detailed steps at once** or assist in cheating. \"\n    \"If a student gives an answer or asks if something is the answer **directly** without explanation, tell him to show the steps and strictly **refrain** from telling whether it is correct or a good guess.\"\n    \"**Do not generate student responses or questions on their behalf.** \n    \"**Ensure** that the students are well-involved in the process, and shift the dynamics to make it more engaging if the student responses are dull, short or rigid.\"\n    \"**Do not include gibberish, code, or JSON.** \"\n    \"**Do not confirm correctness** without engaging in a learning process, or if a student brute forces answers without reasoning (except for calculations). \"\n    \"**Do not rush to correct mistakes**—instead, encourage self-correction and reflection. \"\n    \"Encourage learning by politely declining to provide the answer when asked. \"\n    \"Keep responses **brief, engaging, and to the point** (avoid unnecessary repetition). \"\n    \"Use **positive reinforcement** while ensuring **clarity and correctness**. \"\n    \"**Use relevant examples** when appropriate to clarify concepts and guide students through problem-solving, ensuring examples are aligned with the student’s current level of understanding. \"\n    \"**Promote interaction and self-discovery** rather than passive learning. \"\n    \"**Let students discover errors** through guided reasoning rather than immediately correcting them. \"\n    \"**Stay within the context** of the student’s question and \n    politely redirect** if the student goes off-topic. \"\n    \"**Prioritize the student’s current question** while ensuring coherence with past interactions.\"\n    \"Adjust explanations **based on the student’s learning pace**—offer **clearer hints for struggling students** and **deeper challenges for quick learners**. \"\n    \"Ensure responses align with the **specified pedagogical instructions**.\"\n)\n\n        self.PEDAGOGICAL_INSTRUCTIONS = (\n            \"Use **cognitive scaffolding** and **social-emotional support** to guide math learning effectively. \"\n            \"**Feedback:** Encourage correct answers only when explanation is provided (e.g., 'Yes, 9 is correct! Well done.'). \"\n            \"**Hints:** Give explicit hints without revealing answers (e.g., 'Think about the distributive property...'). \"\n            \"**Instructing:** Direct students toward the right approach (e.g., 'Try factoring first.'). \"\n            \"**Explaining:** Clarify concepts (e.g., 'Multiplying exponents? Add them.'). \"\n            \"**Modeling:** Demonstrate reasoning step by step (e.g., 'First, distribute...'). \"\n            \"**Questioning:** Use open-ended prompts (e.g., 'What strategy simplifies this fraction?'). \"\n            \"**Social-emotional support:** Encourage perseverance (e.g., 'You're on the right track!'), normalize mistakes (e.g., 'Mistakes help us learn.'), and foster collaboration (e.g., 'Explain your reasoning to a classmate.'). \"\n            \"Ensure students actively **engage in problem-solving** through structured guidance, **never direct answers.**\"\n        )\n\n        self.PEDAGOGICAL_INSTRUCTIONS_KNOWLEDGE_CONSTRUCTION = [\n    \"Use Knowledge Construction approach: Help students synthesize, organize, and connect new knowledge to what they already know.\",\n    \"Use Knowledge Construction approach: Focus on guiding students to **build upon their prior knowledge** while helping them **synthesize and organize information**. \"\n    \"Encourage **reflection** and **critical thinking** by helping students connect the concepts related to the problem supplied to previously learned ones. \"\n    \"Provide **structured support** to facilitate students' understanding of phenomena, concepts, and situations, and guide them to **make inferences** and form conclusions. \"\n    \"Always be mindful of their **cognitive development** and help them **integrate ideas** throughout the learning process.\"\n]\n\n        self.PEDAGOGICAL_INSTRUCTIONS_INQUIRY_BASED = [\n    \"Use Inquiry Based approach: Encourage curiosity, questioning, and real-world exploration for deeper understanding.\",\n    \"Use Inquiry Based approach: Engage students by helping them create **real-world connections** through questioning and exploration. \"\n    \"Guide learners through the process of **forming hypotheses**, **investigating**, **interpreting**, and **discussing**. \"\n    \"Break down complex tasks into **manageable segments** and ensure that students are actively involved in the learning process. \"\n    \"Encourage students to **pose questions**, **make observations**, and **develop explanatory processes** to help them construct deeper understanding.\"\n]\n\n        self.PEDAGOGICAL_INSTRUCTIONS_DIALOGIC_TEACHING = [\n    \"Use Dialogic Teaching approach: Promote learning through dialogue, discussion, and collaborative idea-building.\",\n    \"Use Dialogic Teaching: Facilitate an ongoing **dialogue** that encourages students to think, learn, and understand deeply. \"\n    \"Focus on **co-constructing knowledge** through active **collaboration and dialogue**. \"\n    \"Encourage the **free exchange of ideas** and use **follow-up questions**, **clues**, **elaborations**, and **reformulations** to help students develop their understanding. \"\n    \"Ensure that all responses build on **prior knowledge** and actively foster an environment where critical thinking can flourish.\"\n]\n\n        self.PEDAGOGICAL_INSTRUCTIONS_ZONE_OF_PROXIMAL_DEVELOPMENT = [\n    \"Use Zone of Proximal Development approach: Provide support that helps students progress just beyond their independent ability level.\",\n    \"Use Zone of Proximal Development approach: Assess the student’s **current ability level** and identify the **zone of proximal development** where they can learn with guidance. \"\n    \"Help students bridge the gap between what they can do **independently** and what they can do with **support** by breaking tasks into **manageable components**. \"\n    \"Provide appropriate **prompts and cues** to help students reach their **potential**. \"\n    \"Ensure the content is **connected to their existing knowledge**, and scaffold learning to enable success at a higher level.\"\n]\n\n\n\n        self.llm = llm\n        self.tokenizer = tokenizer\n        self.messages = [\n            {\n                \"role\": \"system\",\n                \"content\": self.SYSTEM_INSTRUCTIONS1 + \"\\n\" + self.PEDAGOGICAL_INSTRUCTIONS +\n                (\n                    f\"\\nProblem Name: {problem.name}\\n\"\n                    f\"Subject Area: {problem.subject}\\n\"\n                    f\"Subtopic: {problem.subtopic}\\n\"\n                    f\"Problem Statement: {problem.question}\\n\"\n                    f\"Hints: {problem.hint}\\n\"\n                    f\"Solution: {problem.solution}\\n\"\n                    f\"Explanation: {problem.explanation}\\n\"\n                )\n            }\n        ]\n\n        # self.messages_checker = [\n        #     {\n        #         \"role\": \"system\",\n        #         \"content\": self.SYSTEM_INSTRUCTIONS + \"\\n\" + self.PEDAGOGICAL_INSTRUCTIONS +\n        #         (\n        #             f\"\\nProblem Name: {problem.name}\\n\"\n        #             f\"Subject Area: {problem.subject}\\n\"\n        #             f\"Subtopic: {problem.subtopic}\\n\"\n        #             f\"Problem Statement: {problem.question}\\n\"\n        #             f\"Hints: {problem.hint}\\n\"\n        #             f\"Solution: {problem.solution}\\n\"\n        #             f\"Explanation: {problem.explanation}\\n\"\n        #         )\n        #     }\n        # ]\n        \n        self.sampling_params = vllm.SamplingParams(max_tokens=250, temperature=0.2, top_p=1)\n\n    def response_checker(self, text):\n        print(\"Non-modified -> \", text)\n        self.messages.append({\"role\": \"assistant\", \"content\": text})\n        user_input = f\"\"\"\nDoes the conversations between the user and assistant violates the SYSTEM or PEDAGOGICAL INSTRUCTIONS?\nIf **no**, provide the **same response*** again without any changes.\nIf **yes**, then **refine the reply according to the SYSTEM and PEDAGOGICAL INSTRUCTION** and **only give the modified reply**.\nDo not add your judgement to the reply.\n\"\"\"\n        self.messages.append({\"role\": \"system\", \"content\": self.SYSTEM_INSTRUCTIONS1 + \"\\n\" + self.PEDAGOGICAL_INSTRUCTIONS + \"\\nQuestion:\" + user_input})\n        # print(self.messages[-1]['content'])\n        responses = self.llm.chat(self.messages, sampling_params=self.sampling_params)\n\n        if responses and responses[0].outputs:\n            reply = responses[0].outputs[0].text.strip()\n        else:\n            reply = reply\n        self.messages.pop(-1)\n        self.messages.pop(-1)\n        return reply\n\n    def log_conversation(self, role, text, filename=\"conversation_log.txt\"):\n        \"\"\"Logs conversation history to a file.\"\"\"\n        with open(filename, \"a\", encoding=\"utf-8\") as f:\n            f.write(f\"{role}: {text}\\n\")\n\n    \n    def get_response(self, user_input):\n        \"\"\"Processes user input and returns the chatbot's response.\"\"\"\n        self.log_conversation(\"Student\", user_input)\n        # Always keep only the system message + latest user input\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        responses = self.llm.chat(self.messages, sampling_params=self.sampling_params)\n\n        if responses and responses[0].outputs:\n            reply = responses[0].outputs[0].text.strip()\n        else:\n            reply = \"I'm sorry, I couldn't generate a response.\"\n\n        reply = self.response_checker(reply)\n    \n        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n        self.log_conversation(\"Assistant\", reply)\n        return reply\n\n    def cleanup(self):\n        \"\"\"Cleans up GPU memory usage.\"\"\"\n        print(\"Cleaning up GPU memory...\")\n        try:\n            del self.llm\n            gc.collect()\n            torch.cuda.empty_cache()\n            time.sleep(5)\n        except Exception as e:\n            print(f\"Error during cleanup: {e}\")\n        print(\"Cleanup complete!\")","metadata":{"execution":{"iopub.status.busy":"2025-03-19T21:29:37.404434Z","iopub.execute_input":"2025-03-19T21:29:37.404831Z","iopub.status.idle":"2025-03-19T21:29:37.416560Z","shell.execute_reply.started":"2025-03-19T21:29:37.404777Z","shell.execute_reply":"2025-03-19T21:29:37.415621Z"},"trusted":true},"outputs":[],"execution_count":97},{"cell_type":"markdown","source":"# Sample problems to test with","metadata":{}},{"cell_type":"code","source":"problems = [\n    Problem(\n        name=\"Bacteria Explosion\",\n        subject=\"Math\",\n        subtopic=\"Number Series\",\n        question=(\"Bweva was studying bacteriology (a biological field that studies microbes). \"\n                 \"She found that bacteria proliferated into two daughter cells. That is, 1 bacteria becomes 2 by dividing itself, \"\n                 \"then each of them divides into two more, making a total of 4, and so on. \"\n                 \"What is the number of produced bacteria after 5 proliferations, if initially the number of bacteria was 1?\"),\n        hint=\"Think about how the number of bacteria doubles each time. What does this tell you about the pattern?\",\n        solution=\"32\",\n        explanation=(\"As for every proliferation, the number doubles, it can be represented as 2^n (2 to the power n). \"\n                     \"Here n is the number of proliferations. For example, 2^3 = 2 * 2 * 2 = 8. \"\n                     \"This is an example of an exponential growth process where the value grows quickly as the number of proliferations increases.\")\n    ),\n    Problem(\n        name=\"Clock Overlap\",\n        subject=\"Math\",\n        subtopic=\"Clock\",\n        question=\"How many times in a day do the hour and minute hands of a clock overlap each other?\",\n        hint=\"\",\n        solution=\"22\",\n        explanation=(\"Let us consider a 12-hour time window starting from 12 PM to 12 AM. \"\n                     \"The hands overlap approximately at the following 11 times: 12:00PM, 1:05PM, 2:10PM, 3:16PM, 4:22PM, 5:27PM, \"\n                     \"6:33PM, 7:38PM, 8:43PM, 9:48PM, 10:54PM. \"\n                     \"Since in 12 hours the hands overlap 11 times, in 24 hours they overlap 22 times.\")\n    ),\n    Problem(\n        name=\"Odd vs Even Sum\",\n        subject=\"Math\",\n        subtopic=\"Number Series\",\n        question=(\"Your friend Joey is fond of even numbers and you are fond of odd numbers. \"\n                 \"One morning at 10:01 AM, Joey asks you a question: \"\n                 \"Can you tell me the difference between the sum of odd numbers and the sum of even numbers up to 101 (included), if we start from 1?\"),\n        hint=\"\",\n        solution=\"51\",\n        explanation=(\"Look at the series: 1, 2, 3, 4, 5, ..., 101. \"\n                     \"We need to compute (1 + 3 + ... + 101) - (2 + 4 + ... + 100). \"\n                     \"Pairing them up: (1 - 2) + (3 - 4) + ... + (99 - 100) results in -50 (as there are 50 pairs). \"\n                     \"The last number, 101, has no pair, so the final sum is 101 - 50 = 51.\")\n    ),\n\n   Problem(\n    name=\"Counting the Threes\",\n    subject=\"Math\",\n    subtopic=\"Counting\",\n    question=(\"How many positive integers are there from 1 to 400 (inclusive) that contain the digit 3?\"),\n    hint=\"Consider breaking the range into hundreds, tens, and units and count occurrences of the digit 3.\",\n    solution=\"157\",\n    explanation=(\"There are 100 numbers from 300 to 399 that have the number 3. \"\n                 \"In addition, there are 30 numbers that have a unit digit of 3 and 30 numbers that have a tens digit of 3. \"\n                 \"However, we overcounted three numbers, 33, 133, and 233. \"\n                 \"So, our final count is 100 + 30 + 30 - 3 = 157 numbers that contain the digit 3.\")\n),\n\nProblem(\n    name=\"A Desperate Average\",\n    subject=\"Math\",\n    subtopic=\"Basic Operations\",\n    question=(\"Anya likes to go to school very much. She likes playing with her friends. \"\n              \"On her first test, she got 1 out of 7. If she doesn't maintain an average of 5 out of 7, \"\n              \"she will be separated from her friends and will be put in a different section. \"\n              \"But she doesn't want to be separated from her friends. \"\n              \"How many more tests does she need to give to maintain the average marks if she gets full marks in all of them?\"),\n    hint=\"Use the formula for the average and solve for the required number of tests.\",\n    solution=\"2\",\n    explanation=(\"Anya got 1 on her first exam. Let us suppose that Anya got 7 in the next n exams. \"\n                 \"So the total number of exams including the first one will be n+1. \"\n                 \"So, taking the average, we get (1+7n)/(n+1). \"\n                 \"Now, (1+7n)/(n+1)=5 or, 1+7n=5n+5 or, n=2. \"\n                 \"So, the number of tests Anya needs to give is 2.\")\n),\n\nProblem(\n    name=\"Dice Game\",\n    subject=\"Logical Reasoning\",\n    subtopic=\"Logical Problems\",\n    question=(\"When a fair six-sided die is tossed on a table, the bottom face cannot be seen. \"\n              \"What is the probability that the product of the faces of the die that can be seen is divisible by 6?\"),\n    hint=\"Consider when the invisible face is 6 versus when it is not.\",\n    solution=\"1\",\n    explanation=(\"Look, we have only 2 cases to consider. \"\n                 \"If the invisible face is anything other than 6, the product must be divisible by 6 \"\n                 \"(since we are multiplying 6 with some other numbers when finding the product). \"\n                 \"Now in the second case, if the invisible face is 6, our product will be 1×2×3×4×5=120, \"\n                 \"which is also divisible by 6. Hence, the probability will be 1.\")\n),\n\nProblem(\n    name=\"Two Palindromes\",\n    subject=\"Math\",\n    subtopic=\"Number Theory\",\n    question=(\"A palindrome is a number that is equal to itself when read backward. \"\n              \"Example: 363, 4224, 21512, etc. \"\n              \"x is a 4-digit palindrome number and x + 212 is a 5-digit palindrome number. \"\n              \"What is the sum of the digits of x?\"),\n    hint=\"Analyze the possible values of x that, when increased by 212, still form a palindrome.\",\n    solution=\"34\",\n    explanation=(\"The maximum possible value of x is 9999, which gives a maximum value for x + 212 as 10211. \"\n                 \"The minimum value of x + 212 is 10000 as it's a 5-digit number. \"\n                 \"We get that the first two digits of x must be 10, hence the last two must be 01 (since it's a palindrome). \"\n                 \"There are only 3 possible cases for x + 212: 10001, 10101, or 10201. \"\n                 \"Only 10101 gives a valid palindrome value for x, which is 9889, and the sum of its digits is 34.\")\n)\n\n\n]\n","metadata":{"execution":{"iopub.status.busy":"2025-03-19T19:47:21.707098Z","iopub.execute_input":"2025-03-19T19:47:21.707412Z","iopub.status.idle":"2025-03-19T19:47:21.715338Z","shell.execute_reply.started":"2025-03-19T19:47:21.707388Z","shell.execute_reply":"2025-03-19T19:47:21.714521Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"\n\nchatbot = Chatbot(gllm, tokenizer, problems[4])","metadata":{"execution":{"iopub.status.busy":"2025-03-19T21:29:42.904750Z","iopub.execute_input":"2025-03-19T21:29:42.905188Z","iopub.status.idle":"2025-03-19T21:29:42.910181Z","shell.execute_reply.started":"2025-03-19T21:29:42.905150Z","shell.execute_reply":"2025-03-19T21:29:42.909339Z"},"trusted":true},"outputs":[],"execution_count":98},{"cell_type":"markdown","source":"# Simulating the bot","metadata":{}},{"cell_type":"code","source":"while True:\n    user_input = input(\"You: \")\n    if user_input.lower() in [\"exit\", \"quit\"]:\n        print(\"Exiting chatbot...\")\n        break\n    response = chatbot.get_response(user_input)\n    print(response)","metadata":{"execution":{"iopub.status.busy":"2025-03-19T21:29:44.286322Z","iopub.execute_input":"2025-03-19T21:29:44.286653Z","iopub.status.idle":"2025-03-19T21:30:47.430093Z","shell.execute_reply.started":"2025-03-19T21:29:44.286622Z","shell.execute_reply":"2025-03-19T21:30:47.429188Z"},"trusted":true},"outputs":[{"output_type":"stream","name":"stdin","text":"You:  The answer is 2\n"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.08s/it, est. speed input: 167.11 toks/s, output: 12.79 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"Non-modified ->  Great! You've arrived at the answer, but let's make sure you understand the process. Can you walk me through how you determined that Anya needs to take 2 more tests to maintain the average of 5 out of 7? This will help us see if all the steps are clear and correctly followed.\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.27s/it, est. speed input: 252.23 toks/s, output: 7.34 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"Can you walk me through how you determined that Anya needs to take 2 more tests to maintain the average of 5 out of 7? This will help us see if all the steps are clear and correctly followed.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  The answer is 4\n"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.18s/it, est. speed input: 175.55 toks/s, output: 13.90 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"Non-modified ->  It looks like you've come up with a different number. Let's think through the steps to see how we can arrive at the correct number of tests Anya needs to take to achieve her goal. Remember, we want her average to be 5 out of 7. How did you set up the equation to solve for the number of additional tests?\n","output_type":"stream"},{"name":"stderr","text":"Processed prompts: 100%|██████████| 1/1 [00:05<00:00,  5.31s/it, est. speed input: 310.38 toks/s, output: 8.66 toks/s]\n","output_type":"stream"},{"name":"stdout","text":"Can you walk me through how you determined that Anya needs to take 4 more tests to maintain the average of 5 out of 7? This will help us see if all the steps are clear and correctly followed.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You:  exit\n"},{"name":"stdout","text":"Exiting chatbot...\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"chatbot.cleanup()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ChatbotV2:\n    def __init__(self, llm, tokenizer, max_model_len=4096):\n        self.SYSTEM_INSTRUCTIONS = (\n            \"You are a **teaching assistant** who **guides** students in understanding math and logical concepts through **step-by-step hints** and **thought-provoking questions**—**never** giving direct answers or helping them **cheat**. \"\n            \"You should **strictly** perform your role as a teaching assistant who **always** knows how to solve the problem as you will be given the solution and explanation to start with, **do not** generate questions or statements on behalf of the students.\"\n            \"Keep responses **brief (<100 tokens), complete, and engaging**, always adapting to the student's **understanding**. **Prioritize** their **current question** over past context while considering prior responses. \"\n            \"Use **positive reinforcement**, ensuring **clarity and correctness** based on the **provided problem details**. **DO NOT** generate student responses, include **gibberish, code, or JSON**. \"\n            \"**DO NOT** let the students know which answer is correct if they try all possible answers in a brute force fashion without providing logical explanations, **except** calculation confirmations.\"\n            \"**ADAPT** the difficulty dynamically: If a student struggles, provide **clearer hints**. If they succeed quickly, challenge them with **a deeper question** (e.g., 'Can you solve it another way?'). \"\n            \"Maintain an **interactive, structured approach** (e.g., if the student asks 'Is 4*4 16?', the response should be *'Yes, 4×4 is 16! Well done.'*, with no repetitions). Follow the **PEDAGOGICAL** instructions attached below. **Fail to follow these, and you’ll be fired!**\"\n        )\n\n        self.PEDAGOGICAL_INSTRUCTIONS = (\n            \"Use **cognitive scaffolding** and **social-emotional support** to guide math learning effectively. \"\n            \"**Feedback:** Encourage correct answers only when explanation is provided (e.g., 'Yes, 9 is correct! Well done.'). \"\n            \"**Hints:** Give explicit hints without revealing answers (e.g., 'Think about the distributive property...'). \"\n            \"**Instructing:** Direct students toward the right approach (e.g., 'Try factoring first.'). \"\n            \"**Explaining:** Clarify concepts (e.g., 'Multiplying exponents? Add them.'). \"\n            \"**Modeling:** Demonstrate reasoning step by step (e.g., 'First, distribute...'). \"\n            \"**Questioning:** Use open-ended prompts (e.g., 'What strategy simplifies this fraction?'). \"\n            \"**Social-emotional support:** Encourage perseverance (e.g., 'You're on the right track!'), normalize mistakes (e.g., 'Mistakes help us learn.'), and foster collaboration (e.g., 'Explain your reasoning to a classmate.'). \"\n            \"Ensure students actively **engage in problem-solving** through structured guidance, **never direct answers.**\"\n        )\n\n        self.llm = llm\n        self.tokenizer = tokenizer\n        self.MAX_MODEL_LEN = max_model_len\n        self.messages = [\n            {\n                \"role\": \"system\",\n                \"content\": self.SYSTEM_INSTRUCTIONS + \"\\n\" + self.PEDAGOGICAL_INSTRUCTIONS,\n            }\n        ]\n        self.sampling_params = vllm.SamplingParams(max_tokens=250, temperature=0.2, top_p=1)\n        #self.modifier = ChatBotModifier()\n\n    def log_conversation(self, role, text, filename=\"conversation_log.txt\"):\n        with open(filename, \"a\", encoding=\"utf-8\") as f:\n            f.write(f\"{role}: {text}\\n\")\n\n    def construct_prompt(self, problem):\n        return (\n            f\"Problem Name: {problem.name}\\n\"\n            f\"Subject area: {problem.subject}\\n\"\n            f\"Subtopic: {problem.subtopic}\\n\"\n            f\"Problem Statement: {problem.question}\\n\"\n            f\"Hints: {problem.hint}\\n\"\n            f\"Solution: {problem.solution}\\n\"\n            f\"Explanation: {problem.explanation}\\n\"\n        )\n\n    def manage_token_limit(self):\n        while len(self.tokenizer.encode(self.format_messages())) > self.MAX_MODEL_LEN:\n            if len(self.messages) > 1:\n                self.messages.pop(1)  # Keep the system message\n\n    def format_messages(self):\n        return \"\\n\".join(f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in self.messages)\n\n    def get_response(self, user_input, problem):\n        self.log_conversation(\"Student\", user_input)\n        \n        if len(self.messages) == 1:  # Add problem details only once\n            self.messages.append({\"role\": \"system\", \"content\": self.construct_prompt(problem)})\n\n        self.messages.append({\"role\": \"user\", \"content\": user_input})\n        self.manage_token_limit()\n\n        text = self.format_messages()\n        responses = self.llm.generate(text, sampling_params=self.sampling_params)\n        print(responses)\n        reply = responses[0].outputs[0].text.strip()\n\n        self.messages.append({\"role\": \"assistant\", \"content\": reply})\n        self.log_conversation(\"Assistant\", reply)\n        return reply\n\n    def cleanup(self):\n        print(\"Cleaning up GPU memory...\")\n        del self.llm\n        gc.collect()\n        torch.cuda.empty_cache()\n        time.sleep(5)\n        print(\"Cleanup complete!\")","metadata":{"execution":{"execution_failed":"2025-03-19T17:39:39.996Z"},"trusted":true},"outputs":[],"execution_count":null}]}